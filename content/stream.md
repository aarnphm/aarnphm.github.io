---
id: stream
tags:
  - seed
  - fruit
  - evergreen
description: a microblog within a blog, at the middle of the night.
date: "2025-10-24"
modified: 2025-10-26 23:59:33 GMT-04:00
title: stream
---

## how we talk about god

- [meta]:
  - date: 2025-10-24 01:23:00 GMT-04:00
  - tags:
    - theology
    - life

"God exists" – we say it like we're saying "the table exists." Same grammatical structure, same verb, same declaration of being. But [[thoughts/Wittgenstein|Wittgensteinians]] would tell you these are entirely different language games. Different forms of life enacted through identical syntax.

When someone tells me "I feel God's presence," I wonder what they're actually describing. Not the [[thoughts/Metaphysics|metaphysical]] claim – that's the easy dismissal. But the phenomenology itself. The feeling of being held when no one's holding you. Of mattering when the universe gives no indication you do. Of not being alone in the dark when, empirically, you are.

I've had these feelings. Walking home at 3 AM and suddenly feeling like the street lights are watching over me. Finding a book at exactly the moment I needed it. That uncanny sense of alignment, of pattern, of something speaking directly to you through the noise of existence.

Theism has this ready explanation: providence, divine orchestration, God moving through your life. Clean. Comforting. Complete.

Strip that away and you're left with something harder to name. Not "coincidence" – that's too dismissive of the actual experience. But this persistent feeling that events _mean_ something. Your pattern-matching brain encountering random noise and insisting – _insisting_ – there's signal there.

"God is good" sounds like "water is wet" – a property of an existing thing. But maybe it's closer to "justice is sacred." Not a description but an orientation. A way of being in the world, encoded in grammatical structures that trick you into thinking you're making claims about reality when you're actually describing how you move through it.

Kierkegaard said faith requires [[thoughts/Camus|absurdity]] – you leap beyond reason into belief. But we're all leaping anyway. Every morning. Into relationships we know will end. Into projects that will be forgotten. Into futures that terminate, always, in death.

The absurdity isn't believing without evidence. The absurdity is pouring coffee at 7 AM when you know – really know, not just intellectually but viscerally – that entropy wins. That nothing persists. That the heat death of the universe is coming for everything you've ever loved.

And yet here we are. Making breakfast. Answering emails. Planning for Tuesday.

This isn't Kierkegaardian faith – no transcendent leap into divine arms. But there's still a leap. Acting _as if_ things matter despite having no cosmic guarantee they do. Building despite impermanence. Loving despite loss. Creating despite destruction.

Maybe religious language expresses this fundamental human insistence on continuing. On treating temporary patterns as if they're eternal truths. On finding meaning in noise because meaning-making is what conscious matter does.

The [[/posts/feelings|feeling]] of being held, of mattering, of not being alone – these persist even after you've dismantled the theological framework. They're built into consciousness itself. This need for connection, for significance, for something beyond the merely material. Not because there's a God responding, but because you're the kind of system that generates these feelings when confronting its own existence.

Strip away providence and you're left with pattern-matching systems encountering randomness and desperately trying to read it as text. As message. As meaning.

We can't help it. It's how we're built.

The grammar of theism gives us vocabulary for these experiences. But the experiences themselves? They're what happens when consciousness encounters itself in a universe that doesn't care. When the meaning-making machine meets the meaningless void and keeps making meaning anyway.

Not despite the void. Through it. Because of it.

Generation without guarantee. Creation without cosmic permission. Love without eternal preservation.

---

- [meta]:
  - date: 2025-10-25 16:50:00 GMT-04:00
  - tags:
    - fruit

The visceral feeling of returning to a place you once lived. Those familiar streets you'd walk daily without really noticing. The tiny brick that used to catch your shoe—gone now. New shops sprouting up, the decommissioned church turned taco spot. The coffee shop with its familiar faces, where you just walk by and smile at each other, no words needed. That bar you used to haunt.

What gets me is seeing the old faces, the same constellation of people I used to orbit. Five months in my new Toronto apartment (which I love, truly), but it feels like years have passed. Time does this strange thing – warps around you when you leave. For me, whole epochs have unfolded. For them, Tuesday follows Monday follows Sunday. The uncanny valley of temporal perception: you've moved through time at different speeds, yet here you both are, occupying the same present moment.

---

- [meta]:
  - date: 2025-10-26 23:26:00 GMT-04:00
  - tags:
    - llm

> Why Superhuman AI Would Kill Us All - [Eliezer Yudkowsky](https://www.youtube.com/watch?v=nRvAt4H7d7E)

Yudkowsky's full argument on eschatology isn't productive whatsoever. Feels a lot more science fiction writing, where he claims that this systems will end up "wanting to do their stuff without wanting to take the pills that [we offer to] makes them to do stuff that we _wants_ them to do instead."

Yudkowsky's claim relies on [[thoughts/AGI|superintelligent]] optimizer + misaligned goals = human extinction. Not because it hates us. Because we're made of atoms it can use for something else. Nanotech grey goo. Designer pathogens. Trees that grow computer chips. All physically possible, therefore inevitable once you have sufficient optimization power.

But intelligence in actual systems is jagged, domain-specific, constrained by architecture. AlphaFold is brilliant at protein folding and useless at everything else. GPT-4 can write code but can't reliably count letters. The jump from "good at prediction" to "can design novel molecular machinery from first principles" assumes transfer learning we haven't seen. These very much resembles the old GOFAI vs NFAI arguments. Maybe the argument here is to have a _composition of multiple domain-specific superintelligence systems_ that amplify our life.

The "foom" scenario requires explosive recursive self-improvement, which is obtuse. GPT-6 builds GPT-7, capabilities doubling weekly until godlike intelligence. Architecturally speaking, maybe we figure out something that scales with attention, but it has to be beyond just Transformers, maybe in conjunction with something like JEPA. The argument assumes breakthroughs on demand.

He did mention about a recursive needs of hydrogen, but fwiw physical constraints matters a lot more here. Building nanoassemblers needs: labs, materials, energy, time for experiments. Biology took billions of years of parallel search to reach cells. You can speed that up with intelligence – how much? The argument assumes "enough."

The frame requires that one assume worst case at every branch, assume maximum capability, assume minimal constraints, therefore _doom_. I just don't think that's not how you build things. Real systems fail in boring ways. Scaling laws break. Architectures saturate. The chain from "AI breaks up marriages" to "superintelligence converts biosphere to computronium" requires assumptions that would be rejected in any engineering domain.

> don't build, don't experiment, don't iterate, because any mistake might be the last. That's **not** how we've solved any complex safety problem. Treating everyone who continues working as equivalent to cigarette executives isn't engaging with technical disagreements.

[[thoughts/Alignment]] is hard. I do think that capabilities scales faster than safety. But the response can't be "stop everything and hope treaties hold." It has to be: build better systems, understand current systems deeply, develop alignment that might work. You need feedback loops. You need to learn from failures at scales where failure isn't extinction.
