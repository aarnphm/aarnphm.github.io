---
id: hack-the-north
tags:
  - seed
---

## OpenLLM, and everything about running LLM in production

Running your own LLM, in general, is a rather complicated process. While vendors such as OpenAI, Cohere provide
easy-to-use APIs for calling LLMs; users are migrating away from these platforms due to high costs, lack
of customisation, and privacy concerns. Open-source alternatives, such as LlaMA 2, StarCoder, StableLM, and Falcon, solve these
concerns by giving full control to the users to run these LLMs. Yet, the process of actually running these
LLMs, for many developers, are intimidating. Questions such as: How do I fine-tune my own LLM? How
should I go to write APIs to run inference? Where do I host these models? How do I monitor its performance? are few
that present themselves to be the first barrier to entry for many. This workshop will go into some of the challenges of running open-source LLMs
and propose that maybe, running your own LLM is not as hard as it seems.
