---
date: "2025-10-17"
description: and building a nano inference engine
id: notes
modified: 2025-10-31 06:47:31 GMT-04:00
slides: true
tags:
  - seed
  - workshop
title: supplement to 0.440
transclude:
  title: false
---

see also: [[thoughts/vllm]], [[thoughts/LLMs]], [[thoughts/tsfm/inference-exercise|BYOIE]], [[thoughts/Transformers|Transformers]], [[thoughts/Attention|Attention]], and [[hinterland/nanovllm]]

> The goal is to understand what an inference engine is.
>
> ```bash
> vllm serve -h
> ```

> [!note]
>
> omitting trt-llm, sglang, lmdeploy for simplicity, but the are more/less doing the same thing.
>
> also it is because I work on vLLM ðŸ˜…

s/o: Michael Goin (RedHat), Nick Hill (RedHat), Simon Go (Berkeley)

[@aarnphm_]

## agenda

## inference engine

## `<|endoftext|>`

Thank you for coming, you can find the slides at `https://workshop.aarnphm.xyz/440`
