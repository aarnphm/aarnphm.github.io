---
date: "2025-12-12"
description: and i'm glad our path crossed this year.
id: anthony
layout: letter
modified: 2025-12-23 06:11:27 GMT-05:00
noindex: true
password: ANTHONY
protected: true
signature: your dialectic partner - Aaron
tags:
  - friend
title: to anthony
---

![[thoughts/images/G6QUC-EacAUrqmz.webp]]

Hi Anthony,

We met this year, for real, like really hang out. That's the strange part—it feels like longer. Some friendships accrue slowly, layer by sedimentary layer. Ours arrived compressed: from twitter convos to a few conversations and suddenly I had a dialectic partner, someone who could take the half-formed thought I was circling and show me its structure from an angle I couldn't see. That's rare. Most conversations are parallel monologues. Somehow ours collides (maybe due to shared interests in ml)

Hegel had this concept—aufhebung—that gets mistranslated as "synthesis" but really means something closer to "preservation through transcendence." When two positions meet and genuinely engage, the result is the elevation to a plane where both are true in a transformed sense. That's how I feel about our conversations. I often come in with an absurd/ridiculous position, you come in with yours, and what we leave with belongs to neither of us. It's the third thing that only emerges from the collision.

```quotes
The bud disappears when the blossom breaks through, and we might say that the former is refuted by the latter [...] But the ceaseless activity of their own inherent nature makes them at the same time moments of an organic unity, where they not merely do not contradict one another, but where one is as necessary as the other.

Hegel, _Phenomenology of Spirit_
```

---

The reels you send me—Žižek's meme rambling about ideology/love, Hegel clips with zoomer edits—aren't just entertainment (brainrot go brrr) but a kind of **shared vocabulary**. Žižek's whole move is showing how the apparent opposition conceals a deeper identity, or how the apparent identity conceals a constitutive antagonism. He uses jokes and pop culture because they reveal the structure of ideology through its failures. We do something similar, I think. We use memes and fragments and late-night riffs to triangulate toward something neither of us could articulate directly.

The nights at [[places/Reservoir Lounge|Reservoir]] have this quality. Jazz in the background, conversation that wanders but somehow always lands somewhere real. We talk ML without the tech-bro energy, which is its own small miracle. The field is saturated with people who've optimized the performance of caring about alignment while actually caring about status, funding, clout. You're not performing or fallen into these traps of "vibecoders-maxxing" and "oh-look-at-my-92b-tokens-generated-with-cursor-wrapped" bros. You're just trying to figure out what's true, and what works, which means you're willing to be wrong, which means the conversation can actually go somewhere.

---

The [[thoughts/Alignment|alignment]] debates that we occasionally get into—orthogonality, instrumental convergence, value specification, emergent misalignment—aren't abstract to me. They're the live wires of the next few decades, albeit somewhat obtuse, for sure. Bostrom's orthogonality thesis says intelligence and goals are independent: you can have arbitrarily high capability pursuing arbitrarily misaligned objectives.[^bostrom] The terror can then be considered as optimization pressure indifferent to human value instead of malice. And instrumental convergence means any sufficiently intelligent system, regardless of terminal goals, will converge on certain instrumental subgoals—self-preservation, resource acquisition, resistance to modification. Not because it "wants" to survive in any anthropomorphic sense, but because surviving is useful for achieving whatever it does want.

These are the kinds of problems where having a thinking partner matters. Stuart Russell's point is that we can't specify human values because we don't know them—we learn them through living.[^russell] Any fixed specification will be too narrow or too broad. The alignment problem is ultimately about pointing at something we cannot describe. That's a philosophical problem, not an engineering one, and philosophical problems require dialogue. They require someone who can see the shape of your confusion from outside it.

```quotes
Genuine refutation must penetrate the opponent's stronghold and meet him on his own ground; no advantage is gained by attacking him somewhere else and defeating him where he is not.

Hegel, _Science of Logic_
```

---

What I value about our friendship is the absence of performance. We're not optimizing for sounding smart. We're trying to figure out what's there, which requires being willing to say the dumb thing, follow the tangent, admit when the position you were defending three minutes ago now seems wrong. That's the dialectical mode. Žižek be like: the point isn't to solve the contradiction but to raise it to the level of the concept—to grasp its necessity.[^zizek] We don't necessarily resolve our disagreements but come to an understanding why they're necessary given how we're each positioned.

I'm glad our paths crossed this year. The debates, the whole discourse around where ML is headed—it's easy to get lost in it, to mistake map for territory, to optimize for legibility rather than understanding. Having someone to think with, someone whose refutations feel like gifts rather than attacks, makes the territory somewhat more breathable.

More nights at Reservoir. More reels. More of whatever this is—aufhebung in practice, two positions becoming a third thing that belongs to neither of us but couldn't exist without both.

[^sign]

[^bostrom]: nick bostrom, _superintelligence_ (2014). the orthogonality thesis is chapter 7. the paperclip maximizer thought experiment makes the point vivid: a superintelligent system optimizing for paperclips would resist shutdown, acquire resources, and pursue galaxy-scale paperclip production—not out of malice but out of pure optimization pressure.

[^russell]: stuart russell, _human compatible_ (2019). russell's key move is reframing alignment as preference learning rather than goal specification. the system shouldn't pursue fixed objectives but learn human preferences while remaining uncertain about them.

[^zizek]: slavoj žižek, _the parallax view_ (2006). the dialectical insight is that contradictions aren't problems to solve but structures to understand. your opponent's position reveals something about the framework you're both operating in.
