---
date: "2024-02-12"
description: instruction-tuned language models performing tasks without examples, outperforming larger models through improved zero-shot generalization.
id: zero-shot learning
modified: 2025-10-29 02:16:24 GMT-04:00
tags:
  - llm
title: zero-shot prompting
---

https://arxiv.org/pdf/2109.01652.pdf

The paper argues that zero-shot prompting on a instruction-tuned small language models outperform [[thoughts/LLMs]] systems.

- Instruction-tuning actually improve zero-shot learning performance.
- Mostly tested on FLAN, but show results throughout with GPT-3 and on few reading comprehension dataset.

Honorable mentions include prompt tuning or few-shots prompting
