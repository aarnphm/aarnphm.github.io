---
id: zero-shot learning
tags:
  - llm
date: "2024-02-12"
title: zero-shot prompting
---

[Source](https://arxiv.org/pdf/2109.01652.pdf)

The paper argues that zero-shot prompting on a instruction-tuned small language models outperform [[thoughts/LLMs]] systems.

- Instruction-tuning actually improve zero-shot learning performance.
- Mostly tested on FLAN, but show results throughout with GPT-3 and on few reading comprehension dataset.

Honorable mentions include prompt tuning or few-shots prompting
