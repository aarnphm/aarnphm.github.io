---
date: '2024-02-12'
description: instruction-tuned language models performing tasks without examples, outperforming larger models through improved zero-shot generalization.
id: zero-shot learning
modified: 2026-01-08 14:31:08 GMT-05:00
tags:
  - llm
title: zero-shot prompting
---

https://arxiv.org/abs/2109.01652.pdf

The paper argues that zero-shot prompting on a instruction-tuned small language models outperform [[thoughts/LLMs]] systems.

- Instruction-tuning actually improve zero-shot learning performance.
- Mostly tested on FLAN, but show results throughout with GPT-3 and on few reading comprehension dataset.

Honorable mentions include prompt tuning or few-shots prompting
