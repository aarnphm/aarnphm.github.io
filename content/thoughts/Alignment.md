---
id: Alignment
tags:
  - ml
date: "2024-03-05"
description: and safety-related topics
modified: 2025-01-01 17:23:00 GMT-05:00
title: Alignment
---

See also: [[thoughts/Overton Window|Overton Window]] and this [blog on alignment research](https://openai.com/blog/our-approach-to-alignment-research)

The act of aligning oneself with a particular group or ideology. This can be done for a variety of reasons, including:

- To gain social acceptance
- To gain power
- To gain resources

Often known as a solution to solve "hallucination" in large models token-generation.

> To align a model is simply teaching it to generate tokens that is within the bound of the Overton Window.

The goal is to build a aligned system that help us solve other alignment problems

> Should we build a [[thoughts/ethics|ethical]] aligned systems, or [[thoughts/moral|morally]] aligned systems?

One of [[thoughts/mechanistic interpretability]]'s goal is to [[thoughts/mechanistic interpretability#ablation|ablate]] harmful features

## [[thoughts/design|design]]

See also [[thoughts/Information Theory|Information Theory]]

## RSP

_published by [Anthropic](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf)_

The idea is to create a standard for risk mitigation strategy when AI system advances. Essentially create a scale to judge "how capable a system can cause harm"

![[thoughts/images/alignment-asl-scale.webp]]

