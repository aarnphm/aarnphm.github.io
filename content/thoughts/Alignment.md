---
id: Alignment
tags:
  - seed
  - ml
date: "2024-03-05"
modified: "2024-10-30"
title: Alignment
---

See also: [[thoughts/Overton Window|Overton Window]] and this [blog on alignment research](https://openai.com/blog/our-approach-to-alignment-research)

The act of aligning oneself with a particular group or ideology. This can be done for a variety of reasons, including:

- To gain social acceptance
- To gain power
- To gain resources

Often known as a solution to solve "hallucination" in large models token-generation.

> To align a model is simply teaching it to generate tokens that is within the bound of the Overton Window.

The goal is to build a aligned system that help us solve other alignment problems

> Should we build a [[thoughts/ethics|ethical]] aligned systems, or [[thoughts/moral|morally]] aligned systems?

One of [[thoughts/mechanistic interpretability]]'s goal is to [[thoughts/mechanistic interpretability#ablation|ablate]] harmful features

### [[thoughts/design|design]]

See also [[thoughts/Information Theory|Information Theory]]
