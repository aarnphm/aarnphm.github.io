---
id: Alignment
tags:
  - seed
  - ml
date: "2024-03-05"
title: Alignment
---

See also: [[thoughts/Overton Window|Overton Window]] and this [blog on alignment research](https://openai.com/blog/our-approach-to-alignment-research)

The act of aligning oneself with a particular group or ideology. This can be done for a variety of reasons, including:
- To gain social acceptance
- To gain power
- To gain resources

Often known as a solution to solve "hallucination" in large models token-generation.

> To align a model is simply teaching it to generate tokens that is within the bound of the Overton Window.

The goal is to build a aligned system that help us solve other alignment problems

> Should we build a [[thoughts/ethics|ethical]] aligned systems, or [[thoughts/moral|morally]] aligned systems?
