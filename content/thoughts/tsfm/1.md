---
date: "2025-08-28"
description: and hollistic overview
id: "1"
modified: 2025-10-29 02:15:54 GMT-04:00
socials:
  link: https://tsfm.ca/lecture-one
tags:
  - ml
  - tsfm
title: lecture one
---

See also: [[thoughts/tsfm/lecture-1-exercise]]

## Manifold Hypothesis

Markov (order-1, bigrams) versus uniform distributions

> As dimensionality grows, fractions of meaningful strings shrinks to zero (curse of dimensionality)

Structured data is more compressible; random data is near max entropy. (Entropy readout in text panel; RLE ratio in image panel.)

> [[/tags/ml|ML]]/DL uses data to fit the regularities of this structured slice; we approximate, we donâ€™t enumerate.

![[thoughts/gradient descent]]

![[thoughts/FFN#backpropagation]]

## exercise

given a `Y = X @ W`, with $X \in \mathbb{R}^{N\times D}, W \in \mathbb{R}^{D\times M}, Y\in \mathbb{R}^{N\times M}$, the [[thoughts/Vector calculus#Jacobian matrix|Jacobian]] is:

$$
dW = X^{T}dY, dW = dY W^{T}
$$
