---
id: "1"
tags:
  - ml
  - tsfm
description: and hollistic overview
date: "2025-08-28"
socials:
  link: https://tsfm.ca/lecture-one
modified: 2025-09-16 18:15:39 GMT-04:00
title: lecture one
---

See also: [[thoughts/tsfm/lecture-1-exercise]]

## Manifold Hypothesis

Markov (order-1, bigrams) versus uniform distributions

> As dimensionality grows, fractions of meaningful strings shrinks to zero (curse of dimensionality)

Structured data is more compressible; random data is near max entropy. (Entropy readout in text panel; RLE ratio in image panel.)

> [[/tags/ml|ML]]/DL uses data to fit the regularities of this structured slice; we approximate, we donâ€™t enumerate.

![[thoughts/gradient descent]]

![[thoughts/FFN#backpropagation]]

## exercise

given a `Y = X @ W`, with $X \in \mathbb{R}^{N\times D}, W \in \mathbb{R}^{D\times M}, Y\in \mathbb{R}^{N\times M}$, the [[thoughts/Vector calculus#Jacobian matrix|Jacobian]] is:

$$
dW = X^{T}dY, dW = dY W^{T}
$$
