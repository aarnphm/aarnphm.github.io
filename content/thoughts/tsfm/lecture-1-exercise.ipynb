{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch jax jaxlib"
      ],
      "metadata": {
        "id": "8pCEG_vsr1wR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817de54a-642e-4f79-c05c-bc3e7f6670d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMK3dbs9rksL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Matrix multiplication forward and backward pass implementation.\n",
        "\n",
        "And test correctness with PyTorch and JAX frameworks\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Tuple\n",
        "\n",
        "try:\n",
        "    import jax\n",
        "    import jax.numpy as jnp\n",
        "    from jax import config as jax_config\n",
        "    jax_config.update(\"jax_enable_x64\", True)\n",
        "    JAX_AVAILABLE = True\n",
        "except Exception:\n",
        "    JAX_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        A: Left matrix of shape (m, k)\n",
        "        B: Right matrix of shape (k, n)\n",
        "\n",
        "    Returns:\n",
        "        Result matrix of shape (m, n)\n",
        "    \"\"\"\n",
        "    return np.dot(A, B)\n"
      ],
      "metadata": {
        "id": "InXiXVWd2pYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Y = X @ W\n",
        "\n",
        "    Args:\n",
        "        X: Input matrix of shape (batch_size, input_dim)\n",
        "        W: Weight matrix of shape (input_dim, output_dim)\n",
        "\n",
        "    Returns:\n",
        "        Output matrix of shape (batch_size, output_dim)\n",
        "    \"\"\"\n",
        "    return matmul(X, W)\n",
        "\n",
        "\n",
        "def backward(X: np.ndarray, W: np.ndarray, dY: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Backward pass for matrix multiplication.\n",
        "\n",
        "    Given the gradient of the loss with respect to the output (dY),\n",
        "    computes the gradients with respect to the input (dX) and weights (dW).\n",
        "\n",
        "    For Y = X @ W:\n",
        "    - dW = # FINISH THIS\n",
        "    - dX = # FINISH THIS\n",
        "\n",
        "    Args:\n",
        "        X: Input matrix of shape (batch_size, input_dim)\n",
        "        W: Weight matrix of shape (input_dim, output_dim)\n",
        "        dY: Gradient of loss w.r.t. output, shape (batch_size, output_dim)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (dW, dX) where:\n",
        "        - dW: Gradient w.r.t. weights, shape (input_dim, output_dim)\n",
        "        - dX: Gradient w.r.t. input, shape (batch_size, input_dim)\n",
        "    \"\"\"\n",
        "    dW = np.ones_like(W) # TODO\n",
        "    dX = np.ones_like(X) # TODO\n",
        "\n",
        "    return dW, dX\n"
      ],
      "metadata": {
        "id": "BkysPoOe2m89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check() -> None:\n",
        "    \"\"\"\n",
        "    Main function to test the forward and backward implementations.\n",
        "\n",
        "    Creates random matrices, performs forward and backward passes manually,\n",
        "    then validates the results against PyTorch's autograd.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create random input and weight matrices\n",
        "    batch_size, input_dim, output_dim = 500, 1000, 100\n",
        "    X = np.random.randn(batch_size, input_dim)\n",
        "    W = np.random.randn(input_dim, output_dim)\n",
        "\n",
        "    # Manual forward pass\n",
        "    Y = forward(X, W)\n",
        "\n",
        "    # Create random gradient for backward pass\n",
        "    dY = np.random.randn(batch_size, output_dim)\n",
        "\n",
        "    # Manual backward pass\n",
        "    dW_manual, dX_manual = backward(X, W, dY)\n",
        "\n",
        "    # PyTorch validation\n",
        "    X_torch = torch.tensor(X, dtype=torch.float64, requires_grad=True)\n",
        "    W_torch = torch.tensor(W, dtype=torch.float64, requires_grad=True)\n",
        "    dY_torch = torch.tensor(dY, dtype=torch.float64)\n",
        "\n",
        "    # PyTorch forward pass\n",
        "    Y_torch = torch.matmul(X_torch, W_torch)\n",
        "\n",
        "    # PyTorch backward pass\n",
        "    Y_torch.backward(dY_torch)\n",
        "\n",
        "    # Extract gradients\n",
        "    assert W_torch.grad is not None, \"W_torch.grad should not be None after backward()\"\n",
        "    assert X_torch.grad is not None, \"X_torch.grad should not be None after backward()\"\n",
        "    dW_torch = W_torch.grad.detach().numpy()\n",
        "    dX_torch = X_torch.grad.detach().numpy()\n",
        "\n",
        "    if JAX_AVAILABLE:\n",
        "        X_jax = jnp.asarray(X)\n",
        "        W_jax = jnp.asarray(W)\n",
        "        dY_jax = jnp.asarray(dY)\n",
        "\n",
        "        # Forward\n",
        "        Y_jax = jnp.matmul(X_jax, W_jax)\n",
        "\n",
        "        def f(X_, W_):\n",
        "            return jnp.matmul(X_, W_)\n",
        "\n",
        "        Y_val, vjp_fun = jax.vjp(f, X_jax, W_jax)\n",
        "        dX_jax, dW_jax = vjp_fun(dY_jax)\n",
        "\n",
        "\n",
        "        Y_jax_np = np.asarray(Y_jax)\n",
        "        dW_jax_np = np.asarray(dW_jax)\n",
        "        dX_jax_np = np.asarray(dX_jax)\n",
        "\n",
        "    # Compare results\n",
        "    print(\"Gradient comparisons:\")\n",
        "    print(f\"dW matches PyTorch: {np.allclose(dW_manual, dW_torch, rtol=1e-10)}\")\n",
        "    print(f\"dX matches PyTorch: {np.allclose(dX_manual, dX_torch, rtol=1e-10)}\")\n",
        "\n",
        "    # Print maximum absolute differences\n",
        "    print(f\"\\nMaximum absolute differences:\")\n",
        "    print(f\"dW max diff: {np.max(np.abs(dW_manual - dW_torch))}\")\n",
        "    print(f\"dX max diff: {np.max(np.abs(dX_manual - dX_torch))}\")\n",
        "\n",
        "    if JAX_AVAILABLE:\n",
        "        print(\"\\nJAX forward comparison:\")\n",
        "        print(f\"Y (NumPy vs JAX) matches: {np.allclose(Y, Y_jax_np, rtol=1e-10, atol=0)}\")\n",
        "\n",
        "        print(\"\\nGradient comparisons vs JAX:\")\n",
        "        print(f\"dW (manual vs JAX) matches: {np.allclose(dW_manual, dW_jax_np, rtol=1e-10, atol=0)}\")\n",
        "        print(f\"dX (manual vs JAX) matches: {np.allclose(dX_manual, dX_jax_np, rtol=1e-10, atol=0)}\")\n",
        "\n",
        "        print(\"\\nMaximum absolute differences (manual vs JAX):\")\n",
        "        print(f\"dW max diff: {np.max(np.abs(dW_manual - dW_jax_np))}\")\n",
        "        print(f\"dX max diff: {np.max(np.abs(dX_manual - dX_jax_np))}\")\n",
        "    else:\n",
        "        print(\"\\n[JAX not available] Skipping JAX validation. Install jax & jaxlib to enable this check.\")"
      ],
      "metadata": {
        "id": "PUO5gcqJtZiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check()"
      ],
      "metadata": {
        "id": "y9uVrselt62F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d0a0b6-048f-4a83-92d2-b76e05bdfa09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient comparisons:\n",
            "dW matches PyTorch: False\n",
            "dX matches PyTorch: False\n",
            "\n",
            "Maximum absolute differences:\n",
            "dW max diff: 93.59434845453889\n",
            "dX max diff: 47.27757285580921\n",
            "\n",
            "JAX forward comparison:\n",
            "Y (NumPy vs JAX) matches: True\n",
            "\n",
            "Gradient comparisons vs JAX:\n",
            "dW (manual vs JAX) matches: False\n",
            "dX (manual vs JAX) matches: False\n",
            "\n",
            "Maximum absolute differences (manual vs JAX):\n",
            "dW max diff: 93.59434845453896\n",
            "dX max diff: 47.27757285580921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Cvm5jsHt8lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}