---
id: "5"
tags:
  - ml
  - tsfm
description: by Bharat Venkitesh, and transformers part 2
socials:
  link: https://tsfm.ca/lecture-five
date: "2025-10-02"
modified: 2025-10-02 19:05:59 GMT-04:00
title: lecture five
---

see also [[thoughts/Transformers]], [[thoughts/LLMs]], and [[thoughts/vllm]]

## [[thoughts/Scaling laws|ontological]]

- 1993: [Learning Curves: Asymptotic Values and Rate of Convergence](https://proceedings.neurips.cc/paper/1993/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html) [@NIPS1993_1aa48fc4]
  - Bell Labs
  - $$
    \epsilon_{\text{test}} = a + \frac{b}{l^{\alpha}}, \epsilon_{\text{train}} = - \frac{b}{l^{\beta}}
    $$
- 2012: [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) [@NIPS2012_c399862d]
  - Residual network
  - VGG
- 2020: [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) [@kaplan2020scalinglawsneurallanguage]
  - OpenAI's empirical study on power-law relationships in language model performance
  - Key findings:
    - Loss scales as power-law with model size $N$, dataset size $D$, and compute $C$
    - Performance depends strongly on scale, weakly on model shape
    - $$
      L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}, L(D) \approx \left(\frac{D_c}{D}\right)^{\alpha_D}
      $$
    - Smooth, predictable improvements enable accurate extrapolation
- 2022: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) [@hoffmann2022trainingcomputeoptimallargelanguage]
  - DeepMind's Chinchilla paper
  - Revised Kaplan et al.'s findings: model size and training data should scale equally
  - For compute-optimal training: $N_{opt} \propto C^{0.5}$, $D_{opt} \propto C^{0.5}$
  - Chinchilla (70B params, 1.4T tokens) outperforms Gopher (280B params, 300B tokens)

## power laws

[Power laws](https://en.wikipedia.org/wiki/Power_law) describe relationships where one quantity varies as a power of another: $y = ax^k$. In deep learning, performance metrics often follow power-law relationships with scale.

- 2017: [Deep Learning Scaling is Predictable, Empirically](https://arxiv.org/abs/1712.00409) [@hestness2017deeplearningscalingpredictable]
  - Baidu Research study showing generalization error follows power-law with dataset size across domains
  - Demonstrates predictable scaling across machine translation, language modeling, image classification, and speech recognition
  - Key insight: can extrapolate final model performance from small-scale experiments
  - General form: error $\propto$ (dataset size)$^{-\alpha}$ where $\alpha$ is task-dependent
  - Enables data-driven decisions about whether to gather more data or improve model architecture

### characteristics

- Self-similarity: power laws exhibit scale invariance (no characteristic scale)
- Heavy tails: extreme values more likely than in exponential distributions
- Log-linearity: $\log y = \log a + k \log x$ appears linear in log-log plots
- Predictability: smooth extrapolation enables forecasting from limited data

### implications for training

- Can predict compute requirements for target performance
- Helps determine optimal allocation between model size, data, and training time
- Enables cost-benefit analysis: when to stop scaling vs. architectural improvements
- Informs decisions about data collection vs. algorithmic innovation

## design choices.

> [!question]
> data size affect performance?

> [!question]
> scale data and parameters?

_answer_: Pareto frontier and compute-optimal efficiency

### depth/width scaling

ratio based on $d_{\text{model}}$ versus depth

### architectures

GLU and SwitchTransformers

## joint scaling

https://arxiv.org/abs/1909.12673

cosine learning rate:

- warmup phase
- decay phase

## how do we choose the right hyperparameters?

- parameters non-embedings

## $\mu P$ cheatsheet

## pre-training strategies

> [!note] Data Parallel
> replicates model weights per GPUs.
