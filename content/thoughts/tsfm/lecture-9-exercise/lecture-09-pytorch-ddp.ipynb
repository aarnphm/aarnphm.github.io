{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing Data Parallel Training in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os, math, argparse, time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "@dataclass\n",
        "class TrainCfg:\n",
        "    vocab_size: int = 512\n",
        "    seq_len: int = 64\n",
        "    d_model: int = 256\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 4\n",
        "    d_ff: int = 1024\n",
        "    batch_size: int = 32\n",
        "    steps: int = 50\n",
        "    lr: float = 3e-4\n",
        "    seed: int = 42\n",
        "    dataset_size: int = 8192"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def setup_distributed_notebook(rank, world_size, backend=\"nccl\", master_addr=\"localhost\", master_port=\"12355\"):\n",
        "    os.environ['MASTER_ADDR'] = master_addr\n",
        "    os.environ['MASTER_PORT'] = master_port\n",
        "    \n",
        "    # Initialize the process group\n",
        "    dist.init_process_group(\n",
        "        backend=backend,\n",
        "        init_method=f'env://',\n",
        "        world_size=world_size,\n",
        "        rank=rank\n",
        "    )\n",
        "    \n",
        "    # Set the device for this process\n",
        "    torch.cuda.set_device(rank)\n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    \n",
        "    return rank, rank, world_size, device  # rank, local_rank, world_size, device\n",
        "\n",
        "def cleanup_distributed():\n",
        "    if dist.is_initialized():\n",
        "        dist.barrier()\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "def set_seed(seed: int, rank: int = 0):\n",
        "    # Make RNG streams reproducible but distinct per-rank if desired\n",
        "    torch.manual_seed(seed + rank)\n",
        "    torch.cuda.manual_seed_all(seed + rank)\n",
        "\n",
        "def broadcast_parameters_and_buffers(model, src=0):\n",
        "    \"\"\"Ensure identical model state across ranks without relying on identical RNG.\"\"\"\n",
        "    for p in model.parameters():\n",
        "        dist.broadcast(p.data, src=src)\n",
        "    for b in model.buffers():\n",
        "        dist.broadcast(b.data, src=src)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "class ToySequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generate random sequences of integers\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=8192, seq_len=64, vocab_size=512, seed=1234, device=\"cpu\"):\n",
        "        g = torch.Generator(device=device).manual_seed(seed)\n",
        "        self.data = torch.randint(0, vocab_size, (num_samples, seq_len), generator=g, device=device).cpu()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self): return self.data.size(0)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def sinusoidal_positional_embedding(max_seq, d_model, device=\"cpu\"):\n",
        "    pe = torch.zeros(max_seq, d_model, device=device)\n",
        "    pos = torch.arange(0, max_seq, device=device).unsqueeze(1)\n",
        "    div = torch.exp(torch.arange(0, d_model, 2, device=device) * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(pos * div)\n",
        "    pe[:, 1::2] = torch.cos(pos * div)\n",
        "    return pe  # [T, D]\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=8):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d = d_model; self.h = n_heads; self.dh = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        qkv = self.qkv(x)                               # [B, T, 3D]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        # reshape to [B, H, T, Dh]\n",
        "        q = q.view(B, T, self.h, self.dh).transpose(1, 2)\n",
        "        k = k.view(B, T, self.h, self.dh).transpose(1, 2)\n",
        "        v = v.view(B, T, self.h, self.dh).transpose(1, 2)\n",
        "        # Scaled dot-product attention (causal LM)\n",
        "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
        "        attn = attn.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.proj(attn)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model=256, n_heads=8, d_ff=1024):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = SelfAttention(d_model, n_heads)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size=512, d_model=256, n_heads=8, n_layers=4, d_ff=1024, max_seq=64):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        pe = sinusoidal_positional_embedding(max_seq, d_model)\n",
        "        self.register_buffer(\"pos_emb\", pe, persistent=False)  # buffer, not trained\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):  # idx: [B, T]\n",
        "        B, T = idx.shape\n",
        "        x = self.tok_emb(idx) + self.pos_emb[:T, :].unsqueeze(0)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)  # [B, T, V]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def sync_grads_allreduce(model, world_size):\n",
        "    for p in model.parameters():\n",
        "        if p.grad is None: continue\n",
        "        dist.all_reduce(p.grad, op=dist.ReduceOp.SUM)\n",
        "        p.grad.div_(world_size)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sync_grads_rs_ag(model, world_size, device):\n",
        "    \"\"\"\n",
        "    Ring all-reduce = reduce_scatter + all_gather.\n",
        "    We implement it explicitly over flattened grads to demonstrate both collectives.\n",
        "    \"\"\"\n",
        "    grads = []\n",
        "    shapes = []\n",
        "    for p in model.parameters():\n",
        "        if p.grad is None:\n",
        "            grads.append(torch.zeros(p.numel(), device=device, dtype=p.dtype))\n",
        "        else:\n",
        "            g = p.grad.detach().contiguous().view(-1)\n",
        "            grads.append(g)\n",
        "        shapes.append(p.shape)\n",
        "\n",
        "    flat = torch.cat(grads)  # [N]\n",
        "    n = flat.numel()\n",
        "    pad = (world_size - (n % world_size)) % world_size\n",
        "    if pad:\n",
        "        flat = F.pad(flat, (0, pad))\n",
        "\n",
        "    chunk_size = flat.numel() // world_size\n",
        "    chunks = list(torch.split(flat, chunk_size))\n",
        "    # reduce_scatter: sum corresponding chunks across ranks -> each rank gets its assigned chunk\n",
        "    out_chunk = torch.empty_like(chunks[0])\n",
        "    dist.reduce_scatter(out_chunk, chunks, op=dist.ReduceOp.SUM)  # sum over ranks, scatter by chunk index\n",
        "    out_chunk.div_(world_size)  # average\n",
        "\n",
        "    # all_gather the averaged chunks back to all ranks\n",
        "    gathered = [torch.empty_like(out_chunk) for _ in range(world_size)]\n",
        "    dist.all_gather(gathered, out_chunk)\n",
        "    flat_avg = torch.cat(gathered, dim=0)\n",
        "    if pad:\n",
        "        flat_avg = flat_avg[:n]\n",
        "\n",
        "    # write back into p.grad\n",
        "    offset = 0\n",
        "    for p in model.parameters():\n",
        "        numel = p.numel()\n",
        "        if p.grad is None:\n",
        "            p.grad = flat_avg[offset:offset+numel].view_as(p).clone()\n",
        "        else:\n",
        "            p.grad.copy_(flat_avg[offset:offset+numel].view_as(p.grad))\n",
        "        offset += numel\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def make_model(cfg: TrainCfg):\n",
        "    return TinyTransformerLM(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        d_model=cfg.d_model, n_heads=cfg.n_heads, n_layers=cfg.n_layers, d_ff=cfg.d_ff,\n",
        "        max_seq=cfg.seq_len,\n",
        "    )\n",
        "\n",
        "def iterate_batches(dataset, batch_size, sampler, pin_memory=False, num_workers=0):\n",
        "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
        "                      pin_memory=pin_memory, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "def lm_loss(logits, tokens):\n",
        "    # next-token prediction: targets are tokens[:, 1:]\n",
        "    targets = tokens[:, 1:].contiguous()                      # [B, T-1]\n",
        "    logits = logits[:, :-1, :].contiguous()                   # [B, T-1, V]\n",
        "    B, Tm1, V = logits.shape\n",
        "    return F.cross_entropy(logits.view(B*Tm1, V), targets.view(B*Tm1))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def train_ddp_worker(rank, world_size, cfg, mode=\"ddp\", sync_method=\"allreduce\", work_dir=\"./runs\", save_tag=\"\"):\n",
        "    \"\"\"\n",
        "    Worker function that runs on each GPU process.\n",
        "    This is called by torch.multiprocessing.spawn()\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Setup distributed\n",
        "        rank, local_rank, world_size, device = setup_distributed_notebook(rank, world_size)\n",
        "        \n",
        "        # Set seed\n",
        "        set_seed(cfg.seed, rank)\n",
        "        \n",
        "        # Create model\n",
        "        model = make_model(cfg).to(device)\n",
        "        \n",
        "        # Broadcast initial weights\n",
        "        broadcast_parameters_and_buffers(model, src=0)\n",
        "        dist.barrier()\n",
        "        \n",
        "        # Create dataset\n",
        "        dataset = ToySequenceDataset(\n",
        "            num_samples=cfg.dataset_size, \n",
        "            seq_len=cfg.seq_len,\n",
        "            vocab_size=cfg.vocab_size, \n",
        "            seed=1234\n",
        "        )\n",
        "        sampler = DistributedSampler(\n",
        "            dataset, \n",
        "            num_replicas=world_size, \n",
        "            rank=rank, \n",
        "            shuffle=False, \n",
        "            drop_last=True\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=cfg.batch_size, \n",
        "            sampler=sampler,\n",
        "            pin_memory=True, \n",
        "            num_workers=0, \n",
        "            drop_last=True\n",
        "        )\n",
        "        \n",
        "        # Setup training based on mode\n",
        "        if mode == \"ddp\":\n",
        "            ddp_model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
        "            optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=cfg.lr)\n",
        "            train_model = ddp_model\n",
        "        else:  # manual\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "            train_model = model\n",
        "        \n",
        "        train_model.train()\n",
        "        step = 0\n",
        "        t0 = time.time()\n",
        "        \n",
        "        for batch in loader:\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = train_model(batch)\n",
        "            loss = lm_loss(logits, batch)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Manual gradient sync if needed\n",
        "            if mode == \"manual\":\n",
        "                if sync_method == \"allreduce\":\n",
        "                    sync_grads_allreduce(model, world_size)\n",
        "                else:\n",
        "                    sync_grads_rs_ag(model, world_size, device)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            # Logging\n",
        "            with torch.no_grad():\n",
        "                metrics = torch.tensor([loss.item(), 1.0], device=device)\n",
        "                dist.all_reduce(metrics, op=dist.ReduceOp.SUM)\n",
        "                if rank == 0 and (step % 10 == 0 or step == cfg.steps - 1):\n",
        "                    mean_loss = metrics[0].item() / world_size\n",
        "                    elapsed = time.time() - t0\n",
        "                    print(f\"[{mode}/{sync_method if mode=='manual' else 'default'}] \"\n",
        "                          f\"step {step:04d}  loss {mean_loss:.4f}  time {elapsed:.2f}s\")\n",
        "            \n",
        "            step += 1\n",
        "            if step >= cfg.steps:\n",
        "                break\n",
        "        \n",
        "        # Save checkpoint\n",
        "        if rank == 0:\n",
        "            os.makedirs(work_dir, exist_ok=True)\n",
        "            tag = save_tag if save_tag else f\"{mode}_{sync_method if mode=='manual' else 'default'}\"\n",
        "            checkpoint_path = os.path.join(work_dir, f\"{tag}.pt\")\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(), \n",
        "                \"cfg\": cfg.__dict__\n",
        "            }, checkpoint_path)\n",
        "            print(f\"[{mode}] Saved checkpoint to {checkpoint_path}\")\n",
        "        \n",
        "        dist.barrier()\n",
        "        \n",
        "    finally:\n",
        "        cleanup_distributed()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def train_multi_gpu(\n",
        "    num_gpus=None,\n",
        "    mode=\"ddp\",\n",
        "    sync_method=\"allreduce\", \n",
        "    steps=50,\n",
        "    batch_size=32,\n",
        "    seq_len=64,\n",
        "    vocab_size=512,\n",
        "    work_dir=\"./runs\",\n",
        "    save_tag=\"\",\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    \n",
        "    Args:\n",
        "        num_gpus: Number of GPUs to use (default: all available)\n",
        "        mode: \"ddp\" or \"manual\"\n",
        "        sync_method: \"allreduce\" or \"rs_ag\" (only for manual mode)\n",
        "        steps: Number of training steps\n",
        "        batch_size: Batch size per GPU\n",
        "        seq_len: Sequence length\n",
        "        vocab_size: Vocabulary size\n",
        "        work_dir: Directory to save checkpoints\n",
        "        save_tag: Tag for checkpoint filename\n",
        "        seed: Random seed\n",
        "    \n",
        "    Example:\n",
        "        # In a Jupyter notebook cell:\n",
        "        train_multi_gpu(num_gpus=2, mode=\"ddp\", steps=100)\n",
        "    \"\"\"\n",
        "    if num_gpus is None:\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "    \n",
        "    if num_gpus == 0:\n",
        "        raise RuntimeError(\"No CUDA devices available!\")\n",
        "    \n",
        "    print(f\"\ud83d\ude80 Launching training on {num_gpus} GPUs...\")\n",
        "    print(f\"   Mode: {mode}\")\n",
        "    if mode == \"manual\":\n",
        "        print(f\"   Sync method: {sync_method}\")\n",
        "    print(f\"   Steps: {steps}\")\n",
        "    print(f\"   Batch size per GPU: {batch_size}\")\n",
        "    print()\n",
        "    \n",
        "    cfg = TrainCfg(\n",
        "        vocab_size=vocab_size,\n",
        "        seq_len=seq_len,\n",
        "        batch_size=batch_size,\n",
        "        steps=steps,\n",
        "        seed=seed\n",
        "    )\n",
        "    \n",
        "    ctx = mp.get_context('fork')\n",
        "    processes = []\n",
        "    \n",
        "    for rank in range(num_gpus):\n",
        "        p = ctx.Process(\n",
        "            target=train_ddp_worker,\n",
        "            args=(rank, num_gpus, cfg, mode, sync_method, work_dir, save_tag)\n",
        "        )\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "    \n",
        "    for p in processes:\n",
        "        p.join()\n",
        "        if p.exitcode != 0:\n",
        "            raise RuntimeError(f\"Process failed with exit code {p.exitcode}\")    \n",
        "    \n",
        "    print(f\"\\n\u2705 Training complete!\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "train_multi_gpu()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\ude80 Launching training on 4 GPUs...\n",
            "   Mode: ddp\n",
            "   Steps: 50\n",
            "   Batch size per GPU: 32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ddp/default] step 0000  loss 6.4049  time 2.07s\n",
            "[ddp/default] step 0010  loss 6.2795  time 2.21s\n",
            "[ddp/default] step 0020  loss 6.2496  time 2.33s\n",
            "[ddp/default] step 0030  loss 6.2507  time 2.45s\n",
            "[ddp/default] step 0040  loss 6.2449  time 2.57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ddp/default] step 0049  loss 6.2457  time 2.69s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ddp] Saved checkpoint to ./runs/ddp_default.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}