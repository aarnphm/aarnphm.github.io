{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 09: PyTorch and JAX\n",
        "A primer on compilation, when it's useful and when it's not. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "os.environ[\"TORCH_LOGS\"] = \"output_code, guards, recompiles\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where does compilation really matter? (JAX vs PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "@jax.jit\n",
        "def relu(X):\n",
        "    return jnp.maximum(0, X)\n",
        "    \n",
        "@jax.jit\n",
        "def two_matmuls(X, A, B):\n",
        "    Y = X @ A\n",
        "    Y = relu(Y)\n",
        "    Y = Y @ B\n",
        "\n",
        "    return Y\n",
        "    \n",
        "rng_key = jax.random.PRNGKey(seed=20)\n",
        "X_jax = jax.random.normal(rng_key, (256, 1024))\n",
        "A_jax = jax.random.normal(rng_key, (1024, 2048))\n",
        "B_jax = jax.random.normal(rng_key, (2048, 1024))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JAX Timing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit two_matmuls(X_jax, A_jax, B_jax)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76 \u03bcs \u00b1 45.7 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is suspiciously fast. Why?\n",
        "\n",
        "JAX uses an asynchronous model to avoid Python overheads - this lets Python's control flow go almost uninterrupted [Source](https://docs.jax.dev/en/latest/async_dispatch.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit two_matmuls(X_jax, A_jax, B_jax).block_until_ready()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "205 \u03bcs \u00b1 2.85 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def two_matmuls_pytorch(X, A, B):\n",
        "    Y = X @ A\n",
        "    Y = F.relu(Y)\n",
        "    Y = Y @ B\n",
        "\n",
        "    return Y\n",
        "\n",
        "X = torch.randn(256, 1024).to('cuda')\n",
        "A = torch.randn(1024, 2048).to('cuda')\n",
        "B = torch.randn(2048, 1024).to('cuda')\n",
        "\n",
        "compiled_pt_fn = torch.compile(two_matmuls_pytorch)\n",
        "\n",
        "# warmup\n",
        "_ = compiled_pt_fn(X, A, B)\n",
        "\n",
        "def pt_fn(compiled=False):\n",
        "    if compiled:\n",
        "        Y = compiled_pt_fn(X, A, B)\n",
        "    else:\n",
        "        Y = two_matmuls_pytorch(X, A, B)\n",
        "    torch.cuda.synchronize()\n",
        "    return Y\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] Output code: \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import torch\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import math\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import random\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import os\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import tempfile\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from math import inf, nan\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from cmath import nanj\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch import device, empty_strided\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import triton\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import triton.language as tl\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] aten = torch.ops.aten\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] async_compile = AsyncCompile()\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] # kernel path: /tmp/torchinductor_root/v7/cv7lqgq3xbirtn35dtjemlikt3u5d4s2fkdj6z4g5ezj4o36c26q.py\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] # Topologically Sorted Source Nodes: [Y_1], Original ATen: [aten.relu]\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] # Source node to ATen node mapping:\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] #   Y_1 => relu\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] # Graph fragment:\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] #   %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] triton_poi_fused_relu_0 = async_compile.triton('triton_poi_fused_relu_0', '''\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import triton\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] import triton.language as tl\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] @triton_heuristics.pointwise(\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     size_hints={'x': 524288}, \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     filename=__file__,\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_relu_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6291456}},\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     min_elem_per_thread=0\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] )\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] @triton.jit\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] def triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     xnumel = 524288\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     x0 = xindex\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     tmp0 = tl.load(in_out_ptr0 + (x0), None)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     tmp1 = tl.full([1], 0, tl.int32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     tmp2 = triton_helpers.maximum(tmp1, tmp0)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     tl.store(in_out_ptr0 + (x0), tmp2, None)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] ''', device_str='cuda')\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] async_compile.wait(globals())\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] del async_compile\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] def call(args):\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     arg0_1, arg1_1, arg2_1 = args\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     args.clear()\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     assert_size_stride(arg0_1, (256, 1024), (1024, 1))\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     assert_size_stride(arg1_1, (1024, 2048), (2048, 1))\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     assert_size_stride(arg2_1, (2048, 1024), (1024, 1))\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         torch.cuda.set_device(0)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         buf0 = empty_strided_cuda((256, 2048), (2048, 1), torch.float32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [Y], Original ATen: [aten.mm]\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         del arg0_1\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         del arg1_1\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         buf1 = buf0; del buf0  # reuse\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [Y_1], Original ATen: [aten.relu]\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         stream0 = get_raw_stream(0)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         triton_poi_fused_relu_0.run(buf1, 524288, stream=stream0)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         buf2 = empty_strided_cuda((256, 1024), (1024, 1), torch.float32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [Y_1, Y_2], Original ATen: [aten.relu, aten.mm]\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         extern_kernels.mm(buf1, arg2_1, out=buf2)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         del arg2_1\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]         del buf1\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     return (buf2, )\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     arg0_1 = rand_strided((256, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     arg1_1 = rand_strided((1024, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     arg2_1 = rand_strided((2048, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1, arg2_1])\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] if __name__ == \"__main__\":\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V1023 23:09:25.842000 163 site-packages/torch/_inductor/graph.py:2345] [0/0] [__output_code] \n",
            "V1023 23:09:25.845000 163 site-packages/torch/_inductor/graph.py:2356] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/ln/clnf7sf6dcvvzyarjkdu7wpnhy4xklyoylvpbtelldu7bdjzyrxq.py\n",
            "I1023 23:09:26.521000 163 site-packages/torch/_inductor/graph.py:2317] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/ln/clnf7sf6dcvvzyarjkdu7wpnhy4xklyoylvpbtelldu7bdjzyrxq.py\n",
            "V1023 23:09:26.534000 163 site-packages/torch/_dynamo/guards.py:3064] [0/0] [__guards] GUARDS:\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] \n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] +- RootGuardManager\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:633 in init_ambient_guards\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:621 in init_ambient_guards\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L['A'], accessed_by=FrameLocalsGuardAccessor(key='A', framelocals_idx=1)\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['A'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[1024, 2048], stride=[2048, 1])  # Y = X @ A  # mp/ipykernel_163/1866734929.py:2 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['A'], '_dynamo_dynamic_indices') == False           # Y = X @ A  # mp/ipykernel_163/1866734929.py:2 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['A'], L['B'], L['X'])\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L['B'], accessed_by=FrameLocalsGuardAccessor(key='B', framelocals_idx=2)\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['B'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2048, 1024], stride=[1024, 1])  # Y = Y @ B  # mp/ipykernel_163/1866734929.py:4 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['B'], '_dynamo_dynamic_indices') == False           # Y = Y @ B  # mp/ipykernel_163/1866734929.py:4 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L['X'], accessed_by=FrameLocalsGuardAccessor(key='X', framelocals_idx=0)\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['X'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[256, 1024], stride=[1024, 1])  # Y = X @ A  # mp/ipykernel_163/1866734929.py:2 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['X'], '_dynamo_dynamic_indices') == False           # Y = X @ A  # mp/ipykernel_163/1866734929.py:2 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor('F')\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['F'], 47733929822064)                       # Y = F.relu(Y)  # mp/ipykernel_163/1866734929.py:3 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | | +- GuardManager: source=G['F'].relu, accessed_by=GetAttrGuardAccessor(relu)\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['F'].relu, 47735473206304)                  # Y = F.relu(Y)  # mp/ipykernel_163/1866734929.py:3 in two_matmuls_pytorch\n",
            "V1023 23:09:26.536000 163 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] \n",
            "V1023 23:09:26.548000 163 site-packages/torch/_dynamo/guards.py:2894] [0/0] [__guards] Guard eval latency = 229.45 us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PyTorch Timing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit -n100 -r5 pt_fn()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "183 \u03bcs \u00b1 2.05 \u03bcs per loop (mean \u00b1 std. dev. of 5 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "pt_fn(compiled=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "tensor([[ 1553.5459,  -450.3364,  1118.1101,  ...,  -346.2913,   189.7560,\n          -763.6236],\n        [  725.9422,  -810.8474,  -106.8808,  ...,  -561.0588,  -770.7185,\n          -948.0106],\n        [  777.3891, -1309.3801, -1330.3395,  ...,   272.8092,   662.3004,\n          1540.0090],\n        ...,\n        [  141.1997,   165.5197,  1213.4856,  ...,  -385.2852,   670.5660,\n         -1739.9691],\n        [ -974.1588,  -289.8643,  1005.3350,  ...,   196.1749,   569.9285,\n         -1241.6023],\n        [ -274.6496, -1811.7827,  1221.9260,  ...,  1461.4171,  -200.6417,\n         -1501.7313]], device='cuda:0')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit -n100 -r5 pt_fn(compiled=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "214 \u03bcs \u00b1 4.64 \u03bcs per loop (mean \u00b1 std. dev. of 5 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speedups with a lot of reads and writes to HBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "@jax.jit\n",
        "def chain_jit(x):\n",
        "    return (jnp.tanh(0.1*x + 1.7) * jax.nn.sigmoid(x) + jnp.exp(-x*x)).sum()\n",
        "\n",
        "x = jnp.ones((10_000_000,), dtype=jnp.float32)\n",
        "\n",
        "chain_jit(x).block_until_ready()\n",
        "\n",
        "%timeit chain_jit(x).block_until_ready()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124 \u03bcs \u00b1 1.2 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "x = torch.ones(10_000_000, device=device, dtype=torch.float32)\n",
        "\n",
        "def chain_pt(x):\n",
        "    return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()\n",
        "\n",
        "compiled = torch.compile(chain_pt)  # requires PyTorch 2.x\n",
        "\n",
        "# Warm up\n",
        "_ = compiled(x)\n",
        "if device == 'cuda': torch.cuda.synchronize()\n",
        "\n",
        "# Fair timing (sync on GPU)\n",
        "if device == 'cuda':\n",
        "    %timeit (chain_pt(x)); torch.cuda.synchronize()\n",
        "    %timeit (compiled(x)); torch.cuda.synchronize()\n",
        "else:\n",
        "    %timeit chain_pt(x)\n",
        "    %timeit compiled(x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] Output code: \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # AOT ID: ['1_inference']\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import torch\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import math\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import random\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import os\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import tempfile\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from math import inf, nan\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from cmath import nanj\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch import device, empty_strided\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton.language as tl\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] aten = torch.ops.aten\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] async_compile = AsyncCompile()\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # kernel path: /tmp/torchinductor_root/tq/ctqd7mrkcjr7yx6darqxxwqev5klc3pf62xqcsb3rb2csqyfaqaa.py\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Topologically Sorted Source Nodes: [mul, add, tanh, sigmoid, mul_1, mul_2, neg, exp, add_1, sum_1], Original ATen: [aten.mul, aten.add, aten.tanh, aten.sigmoid, aten.neg, aten.exp, aten.sum]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Source node to ATen node mapping:\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   add => add\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   add_1 => add_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   exp => exp\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul => mul\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul_1 => mul_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul_2 => mul_2\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   neg => neg\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   sigmoid => sigmoid\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   sum_1 => sum_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   tanh => tanh\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Graph fragment:\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 0.1), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, 1.7), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%add,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%arg0_1,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tanh, %sigmoid), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg0_1), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%mul_2,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%neg,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1, %exp), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%add_1,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0 = async_compile.triton('triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0', '''\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton.language as tl\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] @triton_heuristics.reduction(\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     size_hints={'x': 512, 'r0_': 32768},\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     filename=__file__,\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3128, 'r0_': 40000864}}\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] )\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] @triton.jit\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] def triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xnumel = 391\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_numel = 25576\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     rnumel = r0_numel\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xmask = xindex < xnumel\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_base = tl.arange(0, R0_BLOCK)[None, :]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     rbase = r0_base\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     x0 = xindex\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     for r0_offset in range(0, r0_numel, R0_BLOCK):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         r0_index = r0_offset + r0_base\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         r0_mask = r0_index < r0_numel\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         roffset = r0_offset\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         rindex = r0_index\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         r0_1 = r0_index\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp0 = r0_1 + 25576*x0\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp1 = tl.full([1, 1], 10000000, tl.int32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp2 = tmp0 < tmp1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp3 = tl.load(in_ptr0 + (r0_1 + 25576*x0), r0_mask & tmp2 & xmask, eviction_policy='evict_first', other=0.0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp4 = 0.1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp5 = tmp3 * tmp4\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp6 = 1.7\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp7 = tmp5 + tmp6\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp8 = libdevice.tanh(tmp7)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp9 = tl.sigmoid(tmp3)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp10 = tmp8 * tmp9\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp11 = tmp3 * tmp3\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp12 = -tmp11\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp13 = tl_math.exp(tmp12)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp14 = tmp10 + tmp13\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp15 = tl.full(tmp14.shape, 0, tmp14.dtype)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp16 = tl.where(tmp2, tmp14, tmp15)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         tmp19 = _tmp18 + tmp17\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tmp18 = tl.sum(_tmp18, 1)[:, None]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp18, xmask)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] ''', device_str='cuda')\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # kernel path: /tmp/torchinductor_root/f2/cf2kr2vmlmf5ztedrjrgaozwgfz5kl3r5mqn3j3cnr45ijb56xit.py\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Topologically Sorted Source Nodes: [mul, add, tanh, sigmoid, mul_1, mul_2, neg, exp, add_1, sum_1], Original ATen: [aten.mul, aten.add, aten.tanh, aten.sigmoid, aten.neg, aten.exp, aten.sum]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Source node to ATen node mapping:\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   add => add\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   add_1 => add_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   exp => exp\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul => mul\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul_1 => mul_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   mul_2 => mul_2\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   neg => neg\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   sigmoid => sigmoid\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   sum_1 => sum_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   tanh => tanh\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] # Graph fragment:\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 0.1), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, 1.7), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%add,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%arg0_1,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tanh, %sigmoid), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg0_1), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %neg : [num_users=1] = call_function[target=torch.ops.aten.neg.default](args = (%mul_2,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%neg,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1, %exp), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] #   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%add_1,), kwargs = {})\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] triton_per_fused_add_exp_mul_neg_sigmoid_sum_tanh_1 = async_compile.triton('triton_per_fused_add_exp_mul_neg_sigmoid_sum_tanh_1', '''\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] import triton.language as tl\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] @triton_heuristics.persistent_reduction(\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     size_hints={'x': 1, 'r0_': 512},\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     filename=__file__,\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'constexpr', 'r0_numel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {'xnumel': 1}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_exp_mul_neg_sigmoid_sum_tanh_1', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'r0_': 1564}}\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] )\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] @triton.jit\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] def triton_per_fused_add_exp_mul_neg_sigmoid_sum_tanh_1(in_ptr0, out_ptr0, xnumel, r0_numel):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xnumel = 1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     XBLOCK: tl.constexpr = 1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_numel = 391\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     R0_BLOCK: tl.constexpr = 512\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     rnumel = r0_numel\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xindex = tl.full([1], xoffset, tl.int32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     xmask = tl.full([R0_BLOCK], True, tl.int1)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_index = tl.arange(0, R0_BLOCK)[:]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_offset = 0\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_mask = r0_index < r0_numel\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     roffset = r0_offset\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     rindex = r0_index\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     r0_0 = r0_index\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tmp1 = tl.broadcast_to(tmp0, [R0_BLOCK])\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tmp3 = tl.where(r0_mask, tmp1, 0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     tl.store(out_ptr0 + (tl.full([1], 0, tl.int32)), tmp4, None)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] ''', device_str='cuda')\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] async_compile.wait(globals())\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] del async_compile\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] def call(args):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     arg0_1, = args\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     args.clear()\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     assert_size_stride(arg0_1, (10000000, ), (1, ))\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         torch.cuda.set_device(0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         buf1 = empty_strided_cuda((391, ), (1, ), torch.float32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         # Topologically Sorted Source Nodes: [mul, add, tanh, sigmoid, mul_1, mul_2, neg, exp, add_1, sum_1], Original ATen: [aten.mul, aten.add, aten.tanh, aten.sigmoid, aten.neg, aten.exp, aten.sum]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         stream0 = get_raw_stream(0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0.run(arg0_1, buf1, 391, 25576, stream=stream0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         del arg0_1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         buf2 = empty_strided_cuda((), (), torch.float32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         # Topologically Sorted Source Nodes: [mul, add, tanh, sigmoid, mul_1, mul_2, neg, exp, add_1, sum_1], Original ATen: [aten.mul, aten.add, aten.tanh, aten.sigmoid, aten.neg, aten.exp, aten.sum]\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         stream0 = get_raw_stream(0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         triton_per_fused_add_exp_mul_neg_sigmoid_sum_tanh_1.run(buf1, buf2, 1, 391, stream=stream0)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]         del buf1\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     return (buf2, )\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     arg0_1 = rand_strided((10000000, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     fn = lambda: call([arg0_1])\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] if __name__ == \"__main__\":\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V1023 23:10:52.463000 163 site-packages/torch/_inductor/graph.py:2345] [1/0] [__output_code] \n",
            "V1023 23:10:52.467000 163 site-packages/torch/_inductor/graph.py:2356] [1/0] [__output_code] Output code written to: /tmp/torchinductor_root/d7/cd7at4nv26ae7brdnwxpb3v54kq3766ldohntwvjsat2wi53oqnv.py\n",
            "I1023 23:10:53.214000 163 site-packages/torch/_inductor/graph.py:2317] [1/0] [__output_code] Output code written to: /tmp/torchinductor_root/d7/cd7at4nv26ae7brdnwxpb3v54kq3766ldohntwvjsat2wi53oqnv.py\n",
            "V1023 23:10:53.226000 163 site-packages/torch/_dynamo/guards.py:3064] [1/0] [__guards] GUARDS:\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] \n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] TREE_GUARD_MANAGER:\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] +- RootGuardManager\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:633 in init_ambient_guards\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:621 in init_ambient_guards\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[10000000], stride=[1])  # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 47730632442080)                   # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | +- GuardManager: source=G['torch'].exp, accessed_by=GetAttrGuardAccessor(exp)\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].exp, 47730635322240)               # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | +- GuardManager: source=G['torch'].tanh, accessed_by=GetAttrGuardAccessor(tanh)\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].tanh, 47730634581760)              # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | +- GuardManager: source=G['torch'].sigmoid, accessed_by=GetAttrGuardAccessor(sigmoid)\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].sigmoid, 47730634552432)           # return (torch.tanh(0.1*x + 1.7) * torch.sigmoid(x) + torch.exp(-(x*x))).sum()  # mp/ipykernel_163/2887313187.py:5 in chain_pt\n",
            "V1023 23:10:53.227000 163 site-packages/torch/_dynamo/guards.py:2863] [1/0] [__guards] \n",
            "V1023 23:10:53.238000 163 site-packages/torch/_dynamo/guards.py:2894] [1/0] [__guards] Guard eval latency = 190.70 us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "549 \u03bcs \u00b1 135 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n",
            "81.9 \u03bcs \u00b1 127 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!cat /tmp/torchinductor_root/tq/ctqd7mrkcjr7yx6darqxxwqev5klc3pf62xqcsb3rb2csqyfaqaa.py"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "import triton\r\n",
            "import triton.language as tl\r\n",
            "\r\n",
            "from torch._inductor.runtime import triton_helpers, triton_heuristics\r\n",
            "from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\r\n",
            "from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\r\n",
            "triton_helpers.set_driver_to_gpu()\r\n",
            "\r\n",
            "@triton_heuristics.reduction(\r\n",
            "    size_hints={'x': 512, 'r0_': 32768},\r\n",
            "    reduction_hint=ReductionHint.INNER,\r\n",
            "    filename=__file__,\r\n",
            "    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},\r\n",
            "    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3128, 'r0_': 40000864}}\r\n",
            ")\r\n",
            "@triton.jit\r\n",
            "def triton_red_fused_add_exp_mul_neg_sigmoid_sum_tanh_0(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):\r\n",
            "    xnumel = 391\r\n",
            "    r0_numel = 25576\r\n",
            "    rnumel = r0_numel\r\n",
            "    RBLOCK: tl.constexpr = R0_BLOCK\r\n",
            "    xoffset = tl.program_id(0) * XBLOCK\r\n",
            "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\r\n",
            "    xmask = xindex < xnumel\r\n",
            "    r0_base = tl.arange(0, R0_BLOCK)[None, :]\r\n",
            "    rbase = r0_base\r\n",
            "    x0 = xindex\r\n",
            "    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)\r\n",
            "    for r0_offset in range(0, r0_numel, R0_BLOCK):\r\n",
            "        r0_index = r0_offset + r0_base\r\n",
            "        r0_mask = r0_index < r0_numel\r\n",
            "        roffset = r0_offset\r\n",
            "        rindex = r0_index\r\n",
            "        r0_1 = r0_index\r\n",
            "        tmp0 = r0_1 + 25576*x0\r\n",
            "        tmp1 = tl.full([1, 1], 10000000, tl.int32)\r\n",
            "        tmp2 = tmp0 < tmp1\r\n",
            "        tmp3 = tl.load(in_ptr0 + (r0_1 + 25576*x0), r0_mask & tmp2 & xmask, eviction_policy='evict_first', other=0.0)\r\n",
            "        tmp4 = 0.1\r\n",
            "        tmp5 = tmp3 * tmp4\r\n",
            "        tmp6 = 1.7\r\n",
            "        tmp7 = tmp5 + tmp6\r\n",
            "        tmp8 = libdevice.tanh(tmp7)\r\n",
            "        tmp9 = tl.sigmoid(tmp3)\r\n",
            "        tmp10 = tmp8 * tmp9\r\n",
            "        tmp11 = tmp3 * tmp3\r\n",
            "        tmp12 = -tmp11\r\n",
            "        tmp13 = tl_math.exp(tmp12)\r\n",
            "        tmp14 = tmp10 + tmp13\r\n",
            "        tmp15 = tl.full(tmp14.shape, 0, tmp14.dtype)\r\n",
            "        tmp16 = tl.where(tmp2, tmp14, tmp15)\r\n",
            "        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])\r\n",
            "        tmp19 = _tmp18 + tmp17\r\n",
            "        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)\r\n",
            "    tmp18 = tl.sum(_tmp18, 1)[:, None]\r\n",
            "    tl.store(out_ptr0 + (x0), tmp18, xmask)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speedups in MLP with Post Norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "B, S, D, H = 256, 1024, 512, 2048  # batch, model dim, hidden dim\n",
        "\n",
        "@jax.jit\n",
        "def mlp_block_jit(x, w1, b1, w2, b2, gamma, beta, eps=1e-5):\n",
        "    h = x @ w1 + b1\n",
        "    h = jax.nn.gelu(h, approximate=True)\n",
        "    h = h @ w2 + b2\n",
        "    y = x + h\n",
        "    m = y.mean(-1, keepdims=True)\n",
        "    v = ((y - m) ** 2).mean(-1, keepdims=True)\n",
        "    y = (y - m) / jnp.sqrt(v + eps) * gamma + beta\n",
        "    return y\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "x = jax.random.normal(key, (B, S, D), dtype=jnp.float32)\n",
        "w1 = jax.random.normal(key, (D, H), dtype=jnp.float32); b1 = jnp.zeros((H,), jnp.float32)\n",
        "w2 = jax.random.normal(key, (H, D), dtype=jnp.float32); b2 = jnp.zeros((D,), jnp.float32)\n",
        "gamma = jnp.ones((D,), jnp.float32); beta = jnp.zeros((D,), jnp.float32)\n",
        "\n",
        "# warmup compile\n",
        "_ = mlp_block_jit(x, w1, b1, w2, b2, gamma, beta).block_until_ready()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit mlp_block_jit(x, w1, b1, w2, b2, gamma, beta).block_until_ready()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.8 ms \u00b1 16 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "os.environ['XLA_FLAGS'] = \"--xla_dump_to=./xla_dump --xla_dump_hlo_as_text\"\n",
        "mlp_block_jit(x, w1, b1, w2, b2, gamma, beta).block_until_ready()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "Array([[[-1.4833924 ,  0.3511368 ,  0.21911843, ...,  1.4281534 ,\n          0.06670557,  1.1523116 ],\n        [-0.05532628,  0.30686823, -2.0907853 , ..., -0.4386818 ,\n         -0.09272943, -0.37294406],\n        [-0.21030568, -0.65031123, -0.8474163 , ...,  0.9286902 ,\n          0.6720757 ,  1.1566288 ],\n        ...,\n        [ 0.7291884 , -0.17409474, -1.855061  , ...,  0.95941633,\n          1.1517587 ,  1.0067558 ],\n        [-0.40763727, -1.0683631 , -0.8175719 , ...,  0.6376863 ,\n          1.420852  , -0.0816533 ],\n        [-0.89247984, -0.32276464, -0.95661914, ...,  0.40030655,\n         -0.5419413 ,  1.0331538 ]],\n\n       [[ 0.08665023,  1.0070175 ,  0.34418038, ...,  0.9715808 ,\n         -1.7578357 ,  1.0011321 ],\n        [-0.3371217 , -0.7290098 , -0.8793314 , ...,  1.4327533 ,\n         -0.84283054,  0.594506  ],\n        [-1.1071863 ,  0.83588153, -1.2886665 , ...,  1.7351596 ,\n         -0.67959344,  2.0760362 ],\n        ...,\n        [-1.4165086 , -0.01250071, -1.3415816 , ...,  0.588586  ,\n          0.83905715, -0.21655598],\n        [-1.2349527 ,  0.29662225,  0.29726994, ...,  1.3764622 ,\n         -1.016626  ,  0.8842709 ],\n        [-1.0348955 ,  0.3442188 , -2.7820685 , ...,  0.30853575,\n         -1.5865741 , -1.3960618 ]],\n\n       [[ 0.78204656, -0.48800492, -2.2237203 , ...,  0.7215214 ,\n          1.0675687 ,  0.52323574],\n        [ 0.20290816,  1.126426  , -1.3562756 , ...,  2.7230303 ,\n          0.4314841 ,  1.2716556 ],\n        [ 0.97968495,  0.08276878, -1.4414139 , ...,  0.7545694 ,\n          0.1205961 ,  0.61005837],\n        ...,\n        [-1.7652032 ,  1.3331307 ,  0.367819  , ...,  0.34505123,\n          0.91012853,  0.5177587 ],\n        [-1.0667585 , -0.34969956,  0.38857207, ...,  1.4497745 ,\n         -1.0418208 ,  0.3482728 ],\n        [-1.51338   ,  0.68684214, -1.7676276 , ...,  1.9059407 ,\n         -0.5021093 ,  1.2521542 ]],\n\n       ...,\n\n       [[-2.6006272 , -0.04275149, -1.5876188 , ..., -0.24873829,\n         -0.22786021,  1.1792449 ],\n        [-1.8186767 ,  1.2102818 , -0.45068353, ...,  1.5517423 ,\n          0.4918172 ,  0.25052887],\n        [-0.74142885,  1.670427  , -0.98865646, ...,  1.7784518 ,\n         -0.36094218,  1.1749907 ],\n        ...,\n        [-1.2413505 , -0.8350366 , -1.5539995 , ...,  0.35276145,\n         -0.6849729 ,  0.5636569 ],\n        [ 0.08584712,  1.1014177 , -1.0903517 , ...,  0.8254573 ,\n         -1.1060777 ,  0.81444246],\n        [ 0.3625196 ,  0.9305771 , -1.3874989 , ...,  0.38698503,\n          0.0328509 ,  1.3595221 ]],\n\n       [[-0.47911924,  0.25231856, -0.80800843, ...,  0.97893447,\n         -1.7888386 ,  0.23160388],\n        [ 0.9144962 ,  0.80708015, -0.29978666, ...,  1.1037455 ,\n         -0.8008758 ,  0.42008972],\n        [ 0.8079448 ,  0.22959954,  0.39704219, ..., -1.2238505 ,\n         -0.6500786 ,  0.5033116 ],\n        ...,\n        [-1.1888305 , -0.7429054 , -0.9035111 , ...,  1.3589716 ,\n          1.6133654 ,  0.9031925 ],\n        [ 0.06516648, -0.17753597, -1.7301075 , ...,  0.3745673 ,\n         -0.6934611 ,  0.2264269 ],\n        [ 0.17979008, -0.18821509,  0.2603401 , ...,  1.0544428 ,\n         -1.8980342 , -0.37891057]],\n\n       [[-0.6511408 , -0.6401134 , -1.351962  , ...,  0.9124429 ,\n          0.06817078,  1.5129918 ],\n        [-1.4523484 ,  0.33228758,  0.38814127, ..., -0.28190383,\n         -0.22644867,  0.9293559 ],\n        [-0.6641013 ,  0.3045046 , -1.0391467 , ...,  2.9740884 ,\n         -1.4135337 ,  1.0776745 ],\n        ...,\n        [-1.202672  , -0.68220896,  0.84481144, ...,  0.07751226,\n         -0.30229965,  1.2379043 ],\n        [ 0.01456675,  0.48699516, -1.8974427 , ..., -0.14095679,\n          0.3009941 ,  0.92794   ],\n        [ 0.12759277, -0.970297  , -0.78842133, ...,  0.70787144,\n         -1.1071825 ,  0.65892965]]], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "B, S, D, H = 256, 1024, 512, 2048\n",
        "x  = torch.randn(B, S, D, device=device)\n",
        "w1 = torch.randn(D, H, device=device); b1 = torch.zeros(H, device=device)\n",
        "w2 = torch.randn(H, D, device=device); b2 = torch.zeros(D, device=device)\n",
        "gamma = torch.ones(D, device=device);  beta = torch.zeros(D, device=device)\n",
        "\n",
        "def mlp_block_pt(x, w1, b1, w2, b2, gamma, beta, eps=1e-5):\n",
        "    h = x @ w1 + b1\n",
        "    h = F.gelu(h, approximate='tanh')\n",
        "    h = h @ w2 + b2\n",
        "    y = x + h\n",
        "    m = y.mean(dim=-1, keepdim=True)\n",
        "    v = (y - m).pow(2).mean(dim=-1, keepdim=True)\n",
        "    y = (y - m) / torch.sqrt(v + eps) * gamma + beta\n",
        "    return y\n",
        "\n",
        "compiled = torch.compile(mlp_block_pt)  # PyTorch 2.x\n",
        "\n",
        "# warmup\n",
        "_ = compiled(x, w1, b1, w2, b2, gamma, beta)\n",
        "if device == 'cuda': torch.cuda.synchronize()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] Output code: \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # AOT ID: ['2_inference']\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import torch\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import math\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import random\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import os\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import tempfile\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from math import inf, nan\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from cmath import nanj\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch import device, empty_strided\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton.language as tl\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] aten = torch.ops.aten\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] async_compile = AsyncCompile()\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # kernel path: /tmp/torchinductor_root/wj/cwjdxk6jkioa2ctaiariho5gosqwybyvhihnmhnoc43we3psgh6a.py\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Topologically Sorted Source Nodes: [h, h_1], Original ATen: [aten.add, aten.gelu]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Source node to ATen node mapping:\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   h => add\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   h_1 => add_1, add_2, mul, mul_1, mul_2, mul_3, mul_4, mul_5, tanh\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Graph fragment:\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add : [num_users=4] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %arg2_1), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, 0.5), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %add), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %add), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, 0.044715), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add, %mul_2), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_1, 0.7978845608028654), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul_3,), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%tanh, 1), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_4, %add_2), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] triton_poi_fused_add_gelu_0 = async_compile.triton('triton_poi_fused_add_gelu_0', '''\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton.language as tl\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] @triton_heuristics.pointwise(\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     size_hints={'x': 536870912}, \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     filename=__file__,\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_gelu_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6442459136}},\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     min_elem_per_thread=0\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] )\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] @triton.jit\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] def triton_poi_fused_add_gelu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xnumel = 536870912\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     x2 = xindex\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     x0 = (xindex % 2048)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp0 = tl.load(in_out_ptr0 + (x2), None)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp2 = tmp0 + tmp1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp3 = 0.5\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp4 = tmp2 * tmp3\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp5 = tmp2 * tmp2\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp6 = tmp5 * tmp2\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp7 = 0.044715\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp8 = tmp6 * tmp7\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp9 = tmp2 + tmp8\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp10 = 0.7978845608028654\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp11 = tmp9 * tmp10\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp12 = libdevice.tanh(tmp11)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp13 = 1.0\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp14 = tmp12 + tmp13\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp15 = tmp4 * tmp14\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tl.store(in_out_ptr0 + (x2), tmp15, None)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] ''', device_str='cuda')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # kernel path: /tmp/torchinductor_root/aa/caay4btmsp2mfgzwbj6f7u2ypjj37nzmerxot4n227j2olpcn6gf.py\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Topologically Sorted Source Nodes: [h_2, y, m, sub_1, sub, pow_1, v, add_3, sqrt, truediv, mul, y_1], Original ATen: [aten.add, aten.mean, aten.sub, aten.pow, aten.sqrt, aten.div, aten.mul]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Source node to ATen node mapping:\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   add_3 => add_5\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   h_2 => add_3\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   m => mean\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   mul => mul_6\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   pow_1 => pow_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   sqrt => sqrt\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   sub => sub\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   sub_1 => sub_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   truediv => div\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   v => mean_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   y => add_4\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   y_1 => add_6\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] # Graph fragment:\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_3, %arg4_1), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_4 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %add_3), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mean : [num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%add_4, [-1], True), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_4, %mean), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_4, %mean), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%sub, 2), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%add_5,), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%sub_1, %sqrt), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %arg5_1), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] #   %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_6, %arg6_1), kwargs = {})\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1 = async_compile.triton('triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1', '''\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] import triton.language as tl\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] @triton_heuristics.persistent_reduction(\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     size_hints={'x': 262144, 'r0_': 512},\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     filename=__file__,\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 0, 'r0_': 2147489792}}\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] )\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] @triton.jit\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] def triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel):\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xnumel = 262144\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     XBLOCK: tl.constexpr = 1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     r0_numel = 512\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     R0_BLOCK: tl.constexpr = 512\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     rnumel = r0_numel\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xindex = tl.full([1], xoffset, tl.int32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     xmask = tl.full([R0_BLOCK], True, tl.int1)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     r0_index = tl.arange(0, R0_BLOCK)[:]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     r0_offset = 0\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     r0_mask = tl.full([R0_BLOCK], True, tl.int1)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     roffset = r0_offset\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     rindex = r0_index\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     r0_1 = r0_index\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     x0 = xindex\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (r0_1 + 512*x0), None)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp1 = tl.load(in_out_ptr0 + (r0_1 + 512*x0), None)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp2 = tl.load(in_ptr1 + (r0_1), None, eviction_policy='evict_last')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp20 = tl.load(in_ptr2 + (r0_1), None, eviction_policy='evict_last')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp22 = tl.load(in_ptr3 + (r0_1), None, eviction_policy='evict_last')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp3 = tmp1 + tmp2\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp4 = tmp0 + tmp3\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp5 = tl.broadcast_to(tmp4, [R0_BLOCK])\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp5, 0))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp8 = 512.0\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp9 = (tmp7 / tmp8)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp10 = tmp4 - tmp9\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp11 = tmp10 * tmp10\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp12 = tl.broadcast_to(tmp11, [R0_BLOCK])\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp14 = triton_helpers.promote_to_tensor(tl.sum(tmp12, 0))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp15 = (tmp14 / tmp8)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp16 = 1e-05\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp17 = tmp15 + tmp16\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp18 = libdevice.sqrt(tmp17)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp19 = (tmp10 / tmp18)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp21 = tmp19 * tmp20\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tmp23 = tmp21 + tmp22\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     tl.store(in_out_ptr0 + (r0_1 + 512*x0), tmp23, None)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] ''', device_str='cuda')\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] async_compile.wait(globals())\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] del async_compile\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] def call(args):\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1 = args\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     args.clear()\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg0_1, (256, 1024, 512), (524288, 512, 1))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg1_1, (512, 2048), (2048, 1))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg2_1, (2048, ), (1, ))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg3_1, (2048, 512), (512, 1))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg4_1, (512, ), (1, ))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg5_1, (512, ), (1, ))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     assert_size_stride(arg6_1, (512, ), (1, ))\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         torch.cuda.set_device(0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         buf0 = empty_strided_cuda((262144, 2048), (2048, 1), torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         extern_kernels.mm(reinterpret_tensor(arg0_1, (262144, 512), (512, 1), 0), arg1_1, out=buf0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg1_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         buf1 = reinterpret_tensor(buf0, (256, 1024, 2048), (2097152, 2048, 1), 0); del buf0  # reuse\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         # Topologically Sorted Source Nodes: [h, h_1], Original ATen: [aten.add, aten.gelu]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         stream0 = get_raw_stream(0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         triton_poi_fused_add_gelu_0.run(buf1, arg2_1, 536870912, stream=stream0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg2_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         buf2 = empty_strided_cuda((262144, 512), (512, 1), torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         # Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.mm]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         extern_kernels.mm(reinterpret_tensor(buf1, (262144, 2048), (2048, 1), 0), arg3_1, out=buf2)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg3_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del buf1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         buf5 = reinterpret_tensor(buf2, (256, 1024, 512), (524288, 512, 1), 0); del buf2  # reuse\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         # Topologically Sorted Source Nodes: [h_2, y, m, sub_1, sub, pow_1, v, add_3, sqrt, truediv, mul, y_1], Original ATen: [aten.add, aten.mean, aten.sub, aten.pow, aten.sqrt, aten.div, aten.mul]\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         stream0 = get_raw_stream(0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1.run(buf5, arg0_1, arg4_1, arg5_1, arg6_1, 262144, 512, stream=stream0)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg0_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg4_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg5_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]         del arg6_1\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     return (buf5, )\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg0_1 = rand_strided((256, 1024, 512), (524288, 512, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg1_1 = rand_strided((512, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg2_1 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg3_1 = rand_strided((2048, 512), (512, 1), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg4_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg5_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     arg6_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1])\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] if __name__ == \"__main__\":\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V1023 23:14:45.837000 163 site-packages/torch/_inductor/graph.py:2345] [2/0] [__output_code] \n",
            "V1023 23:14:45.840000 163 site-packages/torch/_inductor/graph.py:2356] [2/0] [__output_code] Output code written to: /tmp/torchinductor_root/ge/cgeocwfjblpsir7kadiun3xv7zck5fj3mzh334sfrhk4kd5ohd7a.py\n",
            "I1023 23:14:46.531000 163 site-packages/torch/_inductor/graph.py:2317] [2/0] [__output_code] Output code written to: /tmp/torchinductor_root/ge/cgeocwfjblpsir7kadiun3xv7zck5fj3mzh334sfrhk4kd5ohd7a.py\n",
            "V1023 23:14:46.545000 163 site-packages/torch/_dynamo/guards.py:3064] [2/0] [__guards] GUARDS:\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] \n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] TREE_GUARD_MANAGER:\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] +- RootGuardManager\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:633 in init_ambient_guards\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:621 in init_ambient_guards\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[256, 1024, 512], stride=[524288, 512, 1])  # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['b1'], L['b2'], L['w1'], L['w2'], L['beta'], L['gamma'])\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['b1'], accessed_by=FrameLocalsGuardAccessor(key='b1', framelocals_idx=2)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['b1'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2048], stride=[1])  # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['b1'], '_dynamo_dynamic_indices') == False          # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['b2'], accessed_by=FrameLocalsGuardAccessor(key='b2', framelocals_idx=4)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['b2'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[512], stride=[1])  # h = h @ w2 + b2  # mp/ipykernel_163/3020284281.py:13 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['b2'], '_dynamo_dynamic_indices') == False          # h = h @ w2 + b2  # mp/ipykernel_163/3020284281.py:13 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['w1'], accessed_by=FrameLocalsGuardAccessor(key='w1', framelocals_idx=1)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['w1'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[512, 2048], stride=[2048, 1])  # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['w1'], '_dynamo_dynamic_indices') == False          # h = x @ w1 + b1  # mp/ipykernel_163/3020284281.py:11 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['w2'], accessed_by=FrameLocalsGuardAccessor(key='w2', framelocals_idx=3)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['w2'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2048, 512], stride=[512, 1])  # h = h @ w2 + b2  # mp/ipykernel_163/3020284281.py:13 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['w2'], '_dynamo_dynamic_indices') == False          # h = h @ w2 + b2  # mp/ipykernel_163/3020284281.py:13 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['eps'], accessed_by=FrameLocalsGuardAccessor(key='eps', framelocals_idx=7)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- EQUALS_MATCH: L['eps'] == 1e-05                                             # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['beta'], accessed_by=FrameLocalsGuardAccessor(key='beta', framelocals_idx=6)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['beta'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[512], stride=[1])  # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['beta'], '_dynamo_dynamic_indices') == False        # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=L['gamma'], accessed_by=FrameLocalsGuardAccessor(key='gamma', framelocals_idx=5)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['gamma'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[512], stride=[1])  # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['gamma'], '_dynamo_dynamic_indices') == False       # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- NO_TENSOR_ALIASING\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- GuardManager: source=G['F'], accessed_by=DictGetItemGuardAccessor('F')\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['F'], 47733929822064)                       # h = F.gelu(h, approximate='tanh')  # mp/ipykernel_163/3020284281.py:12 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | +- GuardManager: source=G['F'].gelu, accessed_by=GetAttrGuardAccessor(gelu)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['F'].gelu, 47730636272432)                  # h = F.gelu(h, approximate='tanh')  # mp/ipykernel_163/3020284281.py:12 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 47730632442080)                   # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].sqrt, accessed_by=GetAttrGuardAccessor(sqrt)\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].sqrt, 47730634581360)              # y = (y - m) / torch.sqrt(v + eps) * gamma + beta  # mp/ipykernel_163/3020284281.py:17 in mlp_block_pt\n",
            "V1023 23:14:46.546000 163 site-packages/torch/_dynamo/guards.py:2863] [2/0] [__guards] \n",
            "V1023 23:14:46.561000 163 site-packages/torch/_dynamo/guards.py:2894] [2/0] [__guards] Guard eval latency = 219.51 us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!cat /tmp/torchinductor_root/wj/cwjdxk6jkioa2ctaiariho5gosqwybyvhihnmhnoc43we3psgh6a.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "import triton\r\n",
            "import triton.language as tl\r\n",
            "\r\n",
            "from torch._inductor.runtime import triton_helpers, triton_heuristics\r\n",
            "from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\r\n",
            "from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\r\n",
            "triton_helpers.set_driver_to_gpu()\r\n",
            "\r\n",
            "@triton_heuristics.pointwise(\r\n",
            "    size_hints={'x': 536870912}, \r\n",
            "    filename=__file__,\r\n",
            "    triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},\r\n",
            "    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_gelu_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6442459136}},\r\n",
            "    min_elem_per_thread=0\r\n",
            ")\r\n",
            "@triton.jit\r\n",
            "def triton_poi_fused_add_gelu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\r\n",
            "    xnumel = 536870912\r\n",
            "    xoffset = tl.program_id(0) * XBLOCK\r\n",
            "    xindex = xoffset + tl.arange(0, XBLOCK)[:]\r\n",
            "    xmask = tl.full([XBLOCK], True, tl.int1)\r\n",
            "    x2 = xindex\r\n",
            "    x0 = (xindex % 2048)\r\n",
            "    tmp0 = tl.load(in_out_ptr0 + (x2), None)\r\n",
            "    tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last')\r\n",
            "    tmp2 = tmp0 + tmp1\r\n",
            "    tmp3 = 0.5\r\n",
            "    tmp4 = tmp2 * tmp3\r\n",
            "    tmp5 = tmp2 * tmp2\r\n",
            "    tmp6 = tmp5 * tmp2\r\n",
            "    tmp7 = 0.044715\r\n",
            "    tmp8 = tmp6 * tmp7\r\n",
            "    tmp9 = tmp2 + tmp8\r\n",
            "    tmp10 = 0.7978845608028654\r\n",
            "    tmp11 = tmp9 * tmp10\r\n",
            "    tmp12 = libdevice.tanh(tmp11)\r\n",
            "    tmp13 = 1.0\r\n",
            "    tmp14 = tmp12 + tmp13\r\n",
            "    tmp15 = tmp4 * tmp14\r\n",
            "    tl.store(in_out_ptr0 + (x2), tmp15, None)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!cat /tmp/torchinductor_root/ge/cgeocwfjblpsir7kadiun3xv7zck5fj3mzh334sfrhk4kd5ohd7a.py"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# AOT ID: ['2_inference']\r\n",
            "from ctypes import c_void_p, c_long, c_int\r\n",
            "import torch\r\n",
            "import math\r\n",
            "import random\r\n",
            "import os\r\n",
            "import tempfile\r\n",
            "from math import inf, nan\r\n",
            "from cmath import nanj\r\n",
            "from torch._inductor.hooks import run_intermediate_hooks\r\n",
            "from torch._inductor.utils import maybe_profile\r\n",
            "from torch._inductor.codegen.memory_planning import _align as align\r\n",
            "from torch import device, empty_strided\r\n",
            "from torch._inductor.async_compile import AsyncCompile\r\n",
            "from torch._inductor.select_algorithm import extern_kernels\r\n",
            "import triton\r\n",
            "import triton.language as tl\r\n",
            "from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\r\n",
            "from torch._C import _cuda_getCurrentRawStream as get_raw_stream\r\n",
            "from torch._C import _cuda_getCurrentRawStream as get_raw_stream\r\n",
            "\r\n",
            "aten = torch.ops.aten\r\n",
            "inductor_ops = torch.ops.inductor\r\n",
            "_quantized = torch.ops._quantized\r\n",
            "assert_size_stride = torch._C._dynamo.guards.assert_size_stride\r\n",
            "assert_alignment = torch._C._dynamo.guards.assert_alignment\r\n",
            "empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\r\n",
            "empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\r\n",
            "empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\r\n",
            "reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\r\n",
            "alloc_from_pool = torch.ops.inductor._alloc_from_pool\r\n",
            "async_compile = AsyncCompile()\r\n",
            "empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\r\n",
            "\r\n",
            "\r\n",
            "# kernel path: /tmp/torchinductor_root/wj/cwjdxk6jkioa2ctaiariho5gosqwybyvhihnmhnoc43we3psgh6a.py\r\n",
            "# Topologically Sorted Source Nodes: [h, h_1], Original ATen: [aten.add, aten.gelu]\r\n",
            "# Source node to ATen node mapping:\r\n",
            "#   h => add\r\n",
            "#   h_1 => add_1, add_2, mul, mul_1, mul_2, mul_3, mul_4, mul_5, tanh\r\n",
            "# Graph fragment:\r\n",
            "#   %add : [num_users=4] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_1, %arg2_1), kwargs = {})\r\n",
            "#   %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, 0.5), kwargs = {})\r\n",
            "#   %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %add), kwargs = {})\r\n",
            "#   %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %add), kwargs = {})\r\n",
            "#   %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, 0.044715), kwargs = {})\r\n",
            "#   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add, %mul_2), kwargs = {})\r\n",
            "#   %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_1, 0.7978845608028654), kwargs = {})\r\n",
            "#   %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul_3,), kwargs = {})\r\n",
            "#   %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%tanh, 1), kwargs = {})\r\n",
            "#   %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_4, %add_2), kwargs = {})\r\n",
            "triton_poi_fused_add_gelu_0 = async_compile.triton('triton_poi_fused_add_gelu_0', '''\r\n",
            "import triton\r\n",
            "import triton.language as tl\r\n",
            "\r\n",
            "from torch._inductor.runtime import triton_helpers, triton_heuristics\r\n",
            "from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\r\n",
            "from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\r\n",
            "triton_helpers.set_driver_to_gpu()\r\n",
            "\r\n",
            "@triton_heuristics.pointwise(\r\n",
            "    size_hints={'x': 536870912}, \r\n",
            "    filename=__file__,\r\n",
            "    triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},\r\n",
            "    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_gelu_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6442459136}},\r\n",
            "    min_elem_per_thread=0\r\n",
            ")\r\n",
            "@triton.jit\r\n",
            "def triton_poi_fused_add_gelu_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\r\n",
            "    xnumel = 536870912\r\n",
            "    xoffset = tl.program_id(0) * XBLOCK\r\n",
            "    xindex = xoffset + tl.arange(0, XBLOCK)[:]\r\n",
            "    xmask = tl.full([XBLOCK], True, tl.int1)\r\n",
            "    x2 = xindex\r\n",
            "    x0 = (xindex % 2048)\r\n",
            "    tmp0 = tl.load(in_out_ptr0 + (x2), None)\r\n",
            "    tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last')\r\n",
            "    tmp2 = tmp0 + tmp1\r\n",
            "    tmp3 = 0.5\r\n",
            "    tmp4 = tmp2 * tmp3\r\n",
            "    tmp5 = tmp2 * tmp2\r\n",
            "    tmp6 = tmp5 * tmp2\r\n",
            "    tmp7 = 0.044715\r\n",
            "    tmp8 = tmp6 * tmp7\r\n",
            "    tmp9 = tmp2 + tmp8\r\n",
            "    tmp10 = 0.7978845608028654\r\n",
            "    tmp11 = tmp9 * tmp10\r\n",
            "    tmp12 = libdevice.tanh(tmp11)\r\n",
            "    tmp13 = 1.0\r\n",
            "    tmp14 = tmp12 + tmp13\r\n",
            "    tmp15 = tmp4 * tmp14\r\n",
            "    tl.store(in_out_ptr0 + (x2), tmp15, None)\r\n",
            "''', device_str='cuda')\r\n",
            "\r\n",
            "\r\n",
            "# kernel path: /tmp/torchinductor_root/aa/caay4btmsp2mfgzwbj6f7u2ypjj37nzmerxot4n227j2olpcn6gf.py\r\n",
            "# Topologically Sorted Source Nodes: [h_2, y, m, sub_1, sub, pow_1, v, add_3, sqrt, truediv, mul, y_1], Original ATen: [aten.add, aten.mean, aten.sub, aten.pow, aten.sqrt, aten.div, aten.mul]\r\n",
            "# Source node to ATen node mapping:\r\n",
            "#   add_3 => add_5\r\n",
            "#   h_2 => add_3\r\n",
            "#   m => mean\r\n",
            "#   mul => mul_6\r\n",
            "#   pow_1 => pow_1\r\n",
            "#   sqrt => sqrt\r\n",
            "#   sub => sub\r\n",
            "#   sub_1 => sub_1\r\n",
            "#   truediv => div\r\n",
            "#   v => mean_1\r\n",
            "#   y => add_4\r\n",
            "#   y_1 => add_6\r\n",
            "# Graph fragment:\r\n",
            "#   %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_3, %arg4_1), kwargs = {})\r\n",
            "#   %add_4 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %add_3), kwargs = {})\r\n",
            "#   %mean : [num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%add_4, [-1], True), kwargs = {})\r\n",
            "#   %sub_1 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_4, %mean), kwargs = {})\r\n",
            "#   %sub : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_4, %mean), kwargs = {})\r\n",
            "#   %pow_1 : [num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%sub, 2), kwargs = {})\r\n",
            "#   %mean_1 : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%pow_1, [-1], True), kwargs = {})\r\n",
            "#   %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mean_1, 1e-05), kwargs = {})\r\n",
            "#   %sqrt : [num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%add_5,), kwargs = {})\r\n",
            "#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%sub_1, %sqrt), kwargs = {})\r\n",
            "#   %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %arg5_1), kwargs = {})\r\n",
            "#   %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_6, %arg6_1), kwargs = {})\r\n",
            "triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1 = async_compile.triton('triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1', '''\r\n",
            "import triton\r\n",
            "import triton.language as tl\r\n",
            "\r\n",
            "from torch._inductor.runtime import triton_helpers, triton_heuristics\r\n",
            "from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\r\n",
            "from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\r\n",
            "triton_helpers.set_driver_to_gpu()\r\n",
            "\r\n",
            "@triton_heuristics.persistent_reduction(\r\n",
            "    size_hints={'x': 262144, 'r0_': 512},\r\n",
            "    reduction_hint=ReductionHint.INNER,\r\n",
            "    filename=__file__,\r\n",
            "    triton_meta={'signature': {'in_out_ptr0': '*fp32', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=108, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},\r\n",
            "    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'AA6F4099AE74BF369D45388847321E195BFBEE5A86FCB9B181180FAA5E25C7E8', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 0, 'r0_': 2147489792}}\r\n",
            ")\r\n",
            "@triton.jit\r\n",
            "def triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel):\r\n",
            "    xnumel = 262144\r\n",
            "    XBLOCK: tl.constexpr = 1\r\n",
            "    r0_numel = 512\r\n",
            "    R0_BLOCK: tl.constexpr = 512\r\n",
            "    rnumel = r0_numel\r\n",
            "    RBLOCK: tl.constexpr = R0_BLOCK\r\n",
            "    xoffset = tl.program_id(0) * XBLOCK\r\n",
            "    xindex = tl.full([1], xoffset, tl.int32)\r\n",
            "    xmask = tl.full([R0_BLOCK], True, tl.int1)\r\n",
            "    r0_index = tl.arange(0, R0_BLOCK)[:]\r\n",
            "    r0_offset = 0\r\n",
            "    r0_mask = tl.full([R0_BLOCK], True, tl.int1)\r\n",
            "    roffset = r0_offset\r\n",
            "    rindex = r0_index\r\n",
            "    r0_1 = r0_index\r\n",
            "    x0 = xindex\r\n",
            "    tmp0 = tl.load(in_ptr0 + (r0_1 + 512*x0), None)\r\n",
            "    tmp1 = tl.load(in_out_ptr0 + (r0_1 + 512*x0), None)\r\n",
            "    tmp2 = tl.load(in_ptr1 + (r0_1), None, eviction_policy='evict_last')\r\n",
            "    tmp20 = tl.load(in_ptr2 + (r0_1), None, eviction_policy='evict_last')\r\n",
            "    tmp22 = tl.load(in_ptr3 + (r0_1), None, eviction_policy='evict_last')\r\n",
            "    tmp3 = tmp1 + tmp2\r\n",
            "    tmp4 = tmp0 + tmp3\r\n",
            "    tmp5 = tl.broadcast_to(tmp4, [R0_BLOCK])\r\n",
            "    tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp5, 0))\r\n",
            "    tmp8 = 512.0\r\n",
            "    tmp9 = (tmp7 / tmp8)\r\n",
            "    tmp10 = tmp4 - tmp9\r\n",
            "    tmp11 = tmp10 * tmp10\r\n",
            "    tmp12 = tl.broadcast_to(tmp11, [R0_BLOCK])\r\n",
            "    tmp14 = triton_helpers.promote_to_tensor(tl.sum(tmp12, 0))\r\n",
            "    tmp15 = (tmp14 / tmp8)\r\n",
            "    tmp16 = 1e-05\r\n",
            "    tmp17 = tmp15 + tmp16\r\n",
            "    tmp18 = libdevice.sqrt(tmp17)\r\n",
            "    tmp19 = (tmp10 / tmp18)\r\n",
            "    tmp21 = tmp19 * tmp20\r\n",
            "    tmp23 = tmp21 + tmp22\r\n",
            "    tl.store(in_out_ptr0 + (r0_1 + 512*x0), tmp23, None)\r\n",
            "''', device_str='cuda')\r\n",
            "\r\n",
            "\r\n",
            "async_compile.wait(globals())\r\n",
            "del async_compile\r\n",
            "\r\n",
            "def call(args):\r\n",
            "    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1 = args\r\n",
            "    args.clear()\r\n",
            "    assert_size_stride(arg0_1, (256, 1024, 512), (524288, 512, 1))\r\n",
            "    assert_size_stride(arg1_1, (512, 2048), (2048, 1))\r\n",
            "    assert_size_stride(arg2_1, (2048, ), (1, ))\r\n",
            "    assert_size_stride(arg3_1, (2048, 512), (512, 1))\r\n",
            "    assert_size_stride(arg4_1, (512, ), (1, ))\r\n",
            "    assert_size_stride(arg5_1, (512, ), (1, ))\r\n",
            "    assert_size_stride(arg6_1, (512, ), (1, ))\r\n",
            "    with torch.cuda._DeviceGuard(0):\r\n",
            "        torch.cuda.set_device(0)\r\n",
            "        buf0 = empty_strided_cuda((262144, 2048), (2048, 1), torch.float32)\r\n",
            "        # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\r\n",
            "        extern_kernels.mm(reinterpret_tensor(arg0_1, (262144, 512), (512, 1), 0), arg1_1, out=buf0)\r\n",
            "        del arg1_1\r\n",
            "        buf1 = reinterpret_tensor(buf0, (256, 1024, 2048), (2097152, 2048, 1), 0); del buf0  # reuse\r\n",
            "        # Topologically Sorted Source Nodes: [h, h_1], Original ATen: [aten.add, aten.gelu]\r\n",
            "        stream0 = get_raw_stream(0)\r\n",
            "        triton_poi_fused_add_gelu_0.run(buf1, arg2_1, 536870912, stream=stream0)\r\n",
            "        del arg2_1\r\n",
            "        buf2 = empty_strided_cuda((262144, 512), (512, 1), torch.float32)\r\n",
            "        # Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.mm]\r\n",
            "        extern_kernels.mm(reinterpret_tensor(buf1, (262144, 2048), (2048, 1), 0), arg3_1, out=buf2)\r\n",
            "        del arg3_1\r\n",
            "        del buf1\r\n",
            "        buf5 = reinterpret_tensor(buf2, (256, 1024, 512), (524288, 512, 1), 0); del buf2  # reuse\r\n",
            "        # Topologically Sorted Source Nodes: [h_2, y, m, sub_1, sub, pow_1, v, add_3, sqrt, truediv, mul, y_1], Original ATen: [aten.add, aten.mean, aten.sub, aten.pow, aten.sqrt, aten.div, aten.mul]\r\n",
            "        stream0 = get_raw_stream(0)\r\n",
            "        triton_per_fused_add_div_mean_mul_pow_sqrt_sub_1.run(buf5, arg0_1, arg4_1, arg5_1, arg6_1, 262144, 512, stream=stream0)\r\n",
            "        del arg0_1\r\n",
            "        del arg4_1\r\n",
            "        del arg5_1\r\n",
            "        del arg6_1\r\n",
            "    return (buf5, )\r\n",
            "\r\n",
            "\r\n",
            "def benchmark_compiled_module(times=10, repeat=10):\r\n",
            "    from torch._dynamo.testing import rand_strided\r\n",
            "    from torch._inductor.utils import print_performance\r\n",
            "    arg0_1 = rand_strided((256, 1024, 512), (524288, 512, 1), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg1_1 = rand_strided((512, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg2_1 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg3_1 = rand_strided((2048, 512), (512, 1), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg4_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg5_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\r\n",
            "    arg6_1 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)\r\n",
            "    fn = lambda: call([arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1])\r\n",
            "    return print_performance(fn, times=times, repeat=repeat)\r\n",
            "\r\n",
            "\r\n",
            "if __name__ == \"__main__\":\r\n",
            "    from torch._inductor.wrapper_benchmark import compiled_module_main\r\n",
            "    compiled_module_main('None', benchmark_compiled_module)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%timeit (mlp_block_pt(x, w1, b1, w2, b2, gamma, beta)); torch.cuda.synchronize()\n",
        "%timeit (compiled(x, w1, b1, w2, b2, gamma, beta)); torch.cuda.synchronize()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.5 ms \u00b1 62.8 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n",
            "14.4 ms \u00b1 21.2 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JAX Internals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(jax.make_jaxpr(two_matmuls)(X_jax, A_jax, B_jax))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[256,1024]\u001b[39m b\u001b[35m:f32[1024,2048]\u001b[39m c\u001b[35m:f32[2048,1024]\u001b[39m. \u001b[34;1mlet\n",
            "    \u001b[39;22md\u001b[35m:f32[256,1024]\u001b[39m = jit[\n",
            "      name=two_matmuls\n",
            "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[256,1024]\u001b[39m b\u001b[35m:f32[1024,2048]\u001b[39m c\u001b[35m:f32[2048,1024]\u001b[39m. \u001b[34;1mlet\n",
            "          \u001b[39;22me\u001b[35m:f32[256,2048]\u001b[39m = dot_general[\n",
            "            dimension_numbers=(([1], [0]), ([], []))\n",
            "            preferred_element_type=float32\n",
            "          ] a b\n",
            "          f\u001b[35m:f32[256,2048]\u001b[39m = jit[\n",
            "            name=relu\n",
            "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:f32[256,2048]\u001b[39m. \u001b[34;1mlet\n",
            "                \u001b[39;22mf\u001b[35m:f32[256,2048]\u001b[39m = max 0.0:f32[] e\n",
            "              \u001b[34;1min \u001b[39;22m(f,) }\n",
            "          ] e\n",
            "          d\u001b[35m:f32[256,1024]\u001b[39m = dot_general[\n",
            "            dimension_numbers=(([1], [0]), ([], []))\n",
            "            preferred_element_type=float32\n",
            "          ] f c\n",
            "        \u001b[34;1min \u001b[39;22m(d,) }\n",
            "    ] a b c\n",
            "  \u001b[34;1min \u001b[39;22m(d,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "traced_fn = two_matmuls.trace(X_jax, A_jax, B_jax)\n",
        "print(traced_fn.jaxpr)\n",
        "hlo = traced_fn.lower()\n",
        "print(\"HLO:\\n\", hlo.as_text())\n",
        "hlo_optimized = hlo.compile()\n",
        "print(hlo_optimized.as_text())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[256,1024]\u001b[39m b\u001b[35m:f32[1024,2048]\u001b[39m c\u001b[35m:f32[2048,1024]\u001b[39m. \u001b[34;1mlet\n",
            "    \u001b[39;22md\u001b[35m:f32[256,2048]\u001b[39m = dot_general[\n",
            "      dimension_numbers=(([1], [0]), ([], []))\n",
            "      preferred_element_type=float32\n",
            "    ] a b\n",
            "    e\u001b[35m:f32[256,2048]\u001b[39m = jit[\n",
            "      name=relu\n",
            "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; d\u001b[35m:f32[256,2048]\u001b[39m. \u001b[34;1mlet\n",
            "          \u001b[39;22me\u001b[35m:f32[256,2048]\u001b[39m = max 0.0:f32[] d\n",
            "        \u001b[34;1min \u001b[39;22m(e,) }\n",
            "    ] d\n",
            "    f\u001b[35m:f32[256,1024]\u001b[39m = dot_general[\n",
            "      dimension_numbers=(([1], [0]), ([], []))\n",
            "      preferred_element_type=float32\n",
            "    ] e c\n",
            "  \u001b[34;1min \u001b[39;22m(f,) }\n",
            "HLO:\n",
            " module @jit_two_matmuls attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<256x1024xf32>, %arg1: tensor<1024x2048xf32>, %arg2: tensor<2048x1024xf32>) -> (tensor<256x1024xf32> {jax.result_info = \"result\"}) {\n",
            "    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<256x1024xf32>, tensor<1024x2048xf32>) -> tensor<256x2048xf32>\n",
            "    %1 = call @relu(%0) : (tensor<256x2048xf32>) -> tensor<256x2048xf32>\n",
            "    %2 = stablehlo.dot_general %1, %arg2, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<256x2048xf32>, tensor<2048x1024xf32>) -> tensor<256x1024xf32>\n",
            "    return %2 : tensor<256x1024xf32>\n",
            "  }\n",
            "  func.func private @relu(%arg0: tensor<256x2048xf32>) -> tensor<256x2048xf32> {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<256x2048xf32>\n",
            "    %1 = stablehlo.maximum %0, %arg0 : tensor<256x2048xf32>\n",
            "    return %1 : tensor<256x2048xf32>\n",
            "  }\n",
            "}\n",
            "\n",
            "HloModule jit_two_matmuls, is_scheduled=true, entry_computation_layout={(f32[256,1024]{1,0}, f32[1024,2048]{1,0}, f32[2048,1024]{1,0})->f32[256,1024]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"cf25b161047af3a6d916f493d24923b0\"}\n",
            "\n",
            "%gemm_fusion_dot_general.4_computation (parameter_0: f32[256,1024], parameter_1: f32[1024,2048]) -> f32[256,2048] {\n",
            "  %parameter_0 = f32[256,1024]{1,0} parameter(0)\n",
            "  %parameter_1 = f32[1024,2048]{1,0} parameter(1)\n",
            "  ROOT %dot_general.0 = f32[256,2048]{1,0} dot(%parameter_0, %parameter_1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=7 source_end_line=7 source_column=8 source_end_column=13}\n",
            "}\n",
            "\n",
            "%gemm_fusion_dot_general.11_computation (parameter_0.1: f32[256,2048], parameter_1.1: f32[2048,1024]) -> f32[4,256,1024] {\n",
            "  %parameter_0.1 = f32[256,2048]{1,0} parameter(0)\n",
            "  %constant_1 = f32[] constant(0), metadata={op_name=\"jit(two_matmuls)/jit(relu)\"}\n",
            "  %max.2 = f32[256,2048]{1,0} broadcast(%constant_1), dimensions={}, metadata={op_name=\"jit(two_matmuls)/jit(relu)/max\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=3 source_end_line=3 source_column=11 source_end_column=28}\n",
            "  %max.3 = f32[256,2048]{1,0} maximum(%parameter_0.1, %max.2), metadata={op_name=\"jit(two_matmuls)/jit(relu)/max\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=3 source_end_line=3 source_column=11 source_end_column=28}\n",
            "  %bitcast = f32[256,4,512]{2,1,0} bitcast(%max.3)\n",
            "  %parameter_1.1 = f32[2048,1024]{1,0} parameter(1)\n",
            "  %bitcast.1 = f32[4,512,1024]{2,1,0} bitcast(%parameter_1.1)\n",
            "  ROOT %dot = f32[4,256,1024]{2,1,0} dot(%bitcast, %bitcast.1), lhs_batch_dims={1}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=9 source_end_line=9 source_column=8 source_end_column=13}\n",
            "}\n",
            "\n",
            "%gemm_fusion_dot_general.11.reduce_sub_computation (lhs: f32[], rhs: f32[]) -> f32[] {\n",
            "  %rhs = f32[] parameter(1)\n",
            "  %lhs = f32[] parameter(0)\n",
            "  ROOT %add.1 = f32[] add(%lhs, %rhs)\n",
            "}\n",
            "\n",
            "%fused_reduce (param_0: f32[4,256,1024]) -> f32[256,1024] {\n",
            "  %param_0 = f32[4,256,1024]{2,1,0} parameter(0)\n",
            "  %constant_2_1 = f32[] constant(0)\n",
            "  ROOT %reduce.2 = f32[256,1024]{1,0} reduce(%param_0, %constant_2_1), dimensions={0}, to_apply=%gemm_fusion_dot_general.11.reduce_sub_computation, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=9 source_end_line=9 source_column=8 source_end_column=13}\n",
            "}\n",
            "\n",
            "ENTRY %main.12 (X.1: f32[256,1024], A.2: f32[1024,2048], B.3: f32[2048,1024]) -> f32[256,1024] {\n",
            "  %B.3 = f32[2048,1024]{1,0} parameter(2), metadata={op_name=\"B\"}\n",
            "  %A.2 = f32[1024,2048]{1,0} parameter(1), metadata={op_name=\"A\"}\n",
            "  %X.1 = f32[256,1024]{1,0} parameter(0), metadata={op_name=\"X\"}\n",
            "  %gemm_fusion_dot_general.4 = f32[256,2048]{1,0} fusion(%X.1, %A.2), kind=kCustom, calls=%gemm_fusion_dot_general.4_computation, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=7 source_end_line=7 source_column=8 source_end_column=13}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"64\",\"block_n\":\"32\",\"block_k\":\"32\",\"split_k\":\"1\",\"num_stages\":\"4\",\"num_warps\":\"4\",\"num_ctas\":\"1\",\"is_tma_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
            "  %gemm_fusion_dot_general.11 = f32[4,256,1024]{2,1,0} fusion(%gemm_fusion_dot_general.4, %B.3), kind=kCustom, calls=%gemm_fusion_dot_general.11_computation, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=9 source_end_line=9 source_column=8 source_end_column=13}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"fusion_backend_config\":{\"kind\":\"__triton_gemm\",\"triton_gemm_config\":{\"block_m\":\"64\",\"block_n\":\"64\",\"block_k\":\"64\",\"split_k\":\"4\",\"num_stages\":\"4\",\"num_warps\":\"4\",\"num_ctas\":\"1\",\"is_tma_allowed\":false}},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
            "  ROOT %loop_reduce_fusion = f32[256,1024]{1,0} fusion(%gemm_fusion_dot_general.11), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(two_matmuls)/dot_general\" source_file=\"/tmp/ipykernel_129/221607501.py\" source_line=9 source_end_line=9 source_column=8 source_end_column=13}\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}