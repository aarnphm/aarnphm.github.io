{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer in JAX (functions only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import math\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, value_and_grad, tree_util\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "VOCAB = 512\n",
        "SEQ   = 64\n",
        "DMODEL = 256\n",
        "NHEAD  = 8\n",
        "DFF    = 1024\n",
        "NLAYERS = 4\n",
        "BATCH  = 32\n",
        "STEPS  = 200\n",
        "LR     = 3e-4\n",
        "BETA1  = 0.9\n",
        "BETA2  = 0.999\n",
        "EPS    = 1e-8\n",
        "WEIGHT_DECAY = 0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def glorot(key, shape):\n",
        "    fan_in, fan_out = shape[0], shape[1]\n",
        "    limit = math.sqrt(6.0/(fan_in+fan_out))\n",
        "    return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
        "\n",
        "def zeros(shape): return jnp.zeros(shape, jnp.float32)\n",
        "def ones(shape):  return jnp.ones(shape,  jnp.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1.0 + jnp.tanh(jnp.sqrt(2.0 / jnp.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "def layernorm(x, gamma, beta, eps=1e-5):\n",
        "    mu = x.mean(axis=-1, keepdims=True)\n",
        "    var = jnp.mean((x - mu) ** 2, axis=-1, keepdims=True)\n",
        "    xhat = (x - mu) / jnp.sqrt(var + eps)\n",
        "    return gamma * xhat + beta\n",
        "\n",
        "def sinusoidal_positional_embedding(T, D):\n",
        "    pos = jnp.arange(T, dtype=jnp.float32)[:, None]\n",
        "    i = jnp.arange(0, D, 2, dtype=jnp.float32)[None, :]\n",
        "    div = jnp.exp(-jnp.log(10000.0) * i / D)\n",
        "    pe = jnp.zeros((T, D), dtype=jnp.float32)\n",
        "    pe = pe.at[:, 0::2].set(jnp.sin(pos * div))\n",
        "    pe = pe.at[:, 1::2].set(jnp.cos(pos * div))\n",
        "    return pe  # [T, D]\n",
        "\n",
        "def init_block_params(key, d_model, n_heads, d_ff):\n",
        "    k1, k2, k3, k4 = random.split(key, 4)\n",
        "    # Attention: Wqkv: [D, 3D], Wo: [D, D]\n",
        "    Wqkv = glorot(k1, (d_model, 3 * d_model))\n",
        "    Wo   = glorot(k2, (d_model, d_model))\n",
        "    # MLP: [D, Dff], [Dff], [Dff, D], [D]\n",
        "    W1, b1 = glorot(k3, (d_model, d_ff)), zeros((d_ff,))\n",
        "    W2, b2 = glorot(k4, (d_ff, d_model)), zeros((d_model,))\n",
        "    # LayerNorm scales/biases\n",
        "    ln1_g, ln1_b = ones((d_model,)), zeros((d_model,))\n",
        "    ln2_g, ln2_b = ones((d_model,)), zeros((d_model,))\n",
        "    return {\n",
        "        \"attn\": {\"Wqkv\": Wqkv, \"Wo\": Wo},\n",
        "        \"mlp\":  {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2},\n",
        "        \"ln1\":  {\"g\": ln1_g, \"b\": ln1_b},\n",
        "        \"ln2\":  {\"g\": ln2_g, \"b\": ln2_b},\n",
        "    }\n",
        "\n",
        "def init_params(key, vocab, d_model, n_heads, d_ff, n_layers):\n",
        "    k_tok, k_head, k_ln, *rest = random.split(key, 3 + n_layers)\n",
        "    tok_emb = 0.02 * random.normal(k_tok, (vocab, d_model))\n",
        "    head_W  = glorot(k_head, (d_model, vocab))\n",
        "    head_b  = zeros((vocab,))\n",
        "    ln_g, ln_b = ones((d_model,)), zeros((d_model,))\n",
        "    blocks = [init_block_params(rest[i], d_model, n_heads, d_ff) for i in range(n_layers)]\n",
        "    return {\"tok_emb\": tok_emb, \"blocks\": blocks, \"ln_f\": {\"g\": ln_g, \"b\": ln_b}, \"head\": {\"W\": head_W, \"b\": head_b}}\n",
        "\n",
        "def dense(x, W, b=None):\n",
        "    y = x @ W\n",
        "    return y if b is None else y + b\n",
        "\n",
        "def split_heads(x, n_heads):\n",
        "    # x: [B, T, D] -> [B, H, T, Dh]\n",
        "    B, T, D = x.shape\n",
        "    Dh = D // n_heads\n",
        "    x = x.reshape(B, T, n_heads, Dh)\n",
        "    return jnp.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "def merge_heads(x):\n",
        "    # x: [B, H, T, Dh] -> [B, T, D]\n",
        "    B, H, T, Dh = x.shape\n",
        "    return jnp.transpose(x, (0, 2, 1, 3)).reshape(B, T, H * Dh)\n",
        "\n",
        "def mha_causal(x, Wqkv, Wo, n_heads):\n",
        "    # x: [B, T, D]\n",
        "    B, T, D = x.shape\n",
        "    Dh = D // n_heads\n",
        "    qkv = dense(x, Wqkv)  # [B,T,3D]\n",
        "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
        "    q, k, v = split_heads(q, n_heads), split_heads(k, n_heads), split_heads(v, n_heads)  # [B,H,T,Dh]\n",
        "    scale = 1.0 / math.sqrt(Dh)\n",
        "    scores = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) * scale  # [B,H,T,T]\n",
        "\n",
        "    # causal mask\n",
        "    mask = jnp.tril(jnp.ones((T, T), dtype=jnp.bool_))  # [T,T]\n",
        "    scores = jnp.where(mask, scores, jnp.full_like(scores, -1e30))\n",
        "    attn = jax.nn.softmax(scores, axis=-1)              # [B,H,T,T]\n",
        "    y = jnp.matmul(attn, v)                             # [B,H,T,Dh]\n",
        "    y = merge_heads(y)                                  # [B,T,D]\n",
        "    return dense(y, Wo)                                 # [B,T,D]\n",
        "\n",
        "def transformer_forward(params, tokens):\n",
        "    # tokens: [B, T] int32\n",
        "    B, T = tokens.shape\n",
        "    D = params[\"tok_emb\"].shape[1]\n",
        "    x = params[\"tok_emb\"][tokens]                       # [B,T,D]\n",
        "    x = x + sinusoidal_positional_embedding(T, D)       # broadcast [T,D] -> [B,T,D]\n",
        "\n",
        "    for blk in params[\"blocks\"]:\n",
        "        # Block 1: LN -> MHA -> Residual\n",
        "        y = layernorm(x, blk[\"ln1\"][\"g\"], blk[\"ln1\"][\"b\"])\n",
        "        y = mha_causal(y, blk[\"attn\"][\"Wqkv\"], blk[\"attn\"][\"Wo\"], NHEAD)\n",
        "        x = x + y\n",
        "        # Block 2: LN -> MLP -> Residual\n",
        "        y = layernorm(x, blk[\"ln2\"][\"g\"], blk[\"ln2\"][\"b\"])\n",
        "        y = dense(gelu(dense(y, blk[\"mlp\"][\"W1\"], blk[\"mlp\"][\"b1\"])), blk[\"mlp\"][\"W2\"], blk[\"mlp\"][\"b2\"])\n",
        "        x = x + y\n",
        "\n",
        "    x = layernorm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n",
        "    logits = dense(x, params[\"head\"][\"W\"], params[\"head\"][\"b\"])  # [B,T,V]\n",
        "    return logits\n",
        "\n",
        "def lm_loss(params, tokens):\n",
        "    # Next-token prediction cross-entropy\n",
        "    logits = transformer_forward(params, tokens)         # [B,T,V]\n",
        "    logits = logits[:, :-1, :]                           # [B,T-1,V]\n",
        "    targets = tokens[:, 1:]                              # [B,T-1]\n",
        "    logp = jax.nn.log_softmax(logits, axis=-1)\n",
        "    nll = -jnp.take_along_axis(logp, targets[..., None], axis=-1).squeeze(-1)  # [B,T-1]\n",
        "    return nll.mean()\n",
        "\n",
        "def tree_zeros_like(tree):\n",
        "    return tree_util.tree_map(lambda x: jnp.zeros_like(x), tree)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "\n",
        "def make_opt_state(params):\n",
        "    return {\"m\": tree_zeros_like(params), \"v\": tree_zeros_like(params), \"t\": jnp.array(0, dtype=jnp.int32)}\n",
        "\n",
        "def adamw_update(params, grads, opt_state, lr=LR, beta1=BETA1, beta2=BETA2, eps=EPS, weight_decay=WEIGHT_DECAY):\n",
        "    t = opt_state[\"t\"] + 1\n",
        "    m = tree_util.tree_map(lambda m, g: beta1 * m + (1.0 - beta1) * g, opt_state[\"m\"], grads)\n",
        "    v = tree_util.tree_map(lambda v, g: beta2 * v + (1.0 - beta2) * (g * g), opt_state[\"v\"], grads)\n",
        "    b1c = 1.0 - beta1 ** t.astype(jnp.float32)\n",
        "    b2c = 1.0 - beta2 ** t.astype(jnp.float32)\n",
        "\n",
        "    mhat = tree_util.tree_map(lambda mm: mm / b1c, m)\n",
        "    vhat = tree_util.tree_map(lambda vv: vv / b2c, v)\n",
        "\n",
        "    def _update(p, mh, vh):\n",
        "        upd = mh / (jnp.sqrt(vh) + eps)\n",
        "        if weight_decay != 0.0:\n",
        "            upd = upd + weight_decay * p\n",
        "        return p - lr * upd\n",
        "\n",
        "    new_params = tree_util.tree_map(_update, params, mhat, vhat)\n",
        "    new_state = {\"m\": m, \"v\": v, \"t\": t}\n",
        "    return new_params, new_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "@jit\n",
        "def train_step(params, opt_state, rng):\n",
        "    rng, sub = random.split(rng)\n",
        "    tokens = random.randint(sub, (BATCH, SEQ), 0, VOCAB, dtype=jnp.int32)\n",
        "\n",
        "    def loss_fn(p):\n",
        "        return lm_loss(p, tokens)\n",
        "\n",
        "    loss, grads = value_and_grad(loss_fn)(params)\n",
        "    new_params, new_opt = adamw_update(params, grads, opt_state)\n",
        "    return new_params, new_opt, rng, loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def main():\n",
        "    print(jax.devices())\n",
        "    key = random.PRNGKey(42)\n",
        "    params = init_params(key, VOCAB, DMODEL, NHEAD, DFF, NLAYERS)\n",
        "    opt_state = make_opt_state(params)\n",
        "\n",
        "    # Warmup / compile\n",
        "    t0 = time.time()\n",
        "    params, opt_state, key, loss = train_step(params, opt_state, key)\n",
        "    jax.block_until_ready(loss)\n",
        "    print(f\"Compiled step, initial loss: {float(loss):.4f} (compile took {time.time()-t0:.2f}s)\")\n",
        "\n",
        "    # Train\n",
        "    t0 = time.time()\n",
        "    for s in range(1, STEPS + 1):\n",
        "        params, opt_state, key, loss = train_step(params, opt_state, key)\n",
        "        if s % 20 == 0 or s == STEPS:\n",
        "            jax.block_until_ready(loss)\n",
        "            print(f\"step {s:04d} | loss {float(loss):.4f}\")\n",
        "    print(f\"Done in {time.time()-t0:.2f}s\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}