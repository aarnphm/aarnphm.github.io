{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing Data Parallelism in JAX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "from jax.sharding import Mesh, NamedSharding\n",
        "from jax.sharding import PartitionSpec as P\n",
        "from flax import linen as nn\n",
        "from flax.training.train_state import TrainState\n",
        "import optax\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "@dataclass\n",
        "class TrainCfg:\n",
        "    vocab_size: int = 512\n",
        "    seq_len: int = 64\n",
        "    d_model: int = 256\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 4\n",
        "    d_ff: int = 1024\n",
        "    batch_size: int = 32          # global batch (will be sharded across devices)\n",
        "    steps: int = 50\n",
        "    lr: float = 3e-4\n",
        "    seed: int = 42\n",
        "    dataset_size: int = 8192      # total tokens sequences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "\n",
        "def make_toy_dataset(cfg: TrainCfg, rng_seed: int = 1234):\n",
        "    rs = np.random.RandomState(rng_seed)\n",
        "    data = rs.randint(0, cfg.vocab_size, size=(cfg.dataset_size, cfg.seq_len), dtype=np.int32)\n",
        "    return data  # np.array on host (we'll feed batches each step)\n",
        "\n",
        "def batch_iter(data: np.ndarray, batch_size: int):\n",
        "    n = (len(data) // batch_size) * batch_size\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield data[i:i+batch_size]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def sinusoidal_positional_embedding(max_len, d_model):\n",
        "    pos = jnp.arange(max_len)[:, None]\n",
        "    i = jnp.arange(0, d_model, 2)[None, :]\n",
        "    inv_freq = jnp.exp(-(jnp.log(10000.0) / d_model) * i)\n",
        "    pe = jnp.zeros((max_len, d_model), dtype=jnp.float32)\n",
        "    pe = pe.at[:, 0::2].set(jnp.sin(pos * inv_freq))\n",
        "    pe = pe.at[:, 1::2].set(jnp.cos(pos * inv_freq))\n",
        "    return pe\n",
        "\n",
        "class Block(nn.Module):\n",
        "    d_model: int\n",
        "    n_heads: int\n",
        "    d_ff: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, *, deterministic: bool = True):\n",
        "        y = nn.LayerNorm()(x)\n",
        "        y = lax.with_sharding_constraint(y, P(\"data\", None))\n",
        "        y = nn.SelfAttention(\n",
        "            num_heads=self.n_heads,\n",
        "            qkv_features=self.d_model,\n",
        "            use_bias=False,\n",
        "            dropout_rate=0.0,\n",
        "            deterministic=deterministic,\n",
        "            decode=False,\n",
        "            broadcast_dropout=False,\n",
        "            dtype=jnp.float32,\n",
        "        )(y)\n",
        "        x = x + y\n",
        "        y = nn.LayerNorm()(x)\n",
        "        y = nn.gelu(nn.Dense(self.d_ff)(y))\n",
        "        y = nn.Dense(self.d_model)(y)\n",
        "        return x + y\n",
        "\n",
        "class TinyTransformerLM(nn.Module):\n",
        "    vocab_size: int\n",
        "    d_model: int\n",
        "    n_heads: int\n",
        "    n_layers: int\n",
        "    d_ff: int\n",
        "    max_seq: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.tok_emb = nn.Embed(self.vocab_size, self.d_model)\n",
        "        self.pos_emb = self.param(\n",
        "            \"pos_emb\",\n",
        "            lambda key: sinusoidal_positional_embedding(self.max_seq, self.d_model),\n",
        "        )\n",
        "        self.blocks = [Block(self.d_model, self.n_heads, self.d_ff) for _ in range(self.n_layers)]\n",
        "        self.ln_f = nn.LayerNorm()\n",
        "        self.head = nn.Dense(self.vocab_size, use_bias=False)\n",
        "\n",
        "    def __call__(self, tokens, *, deterministic: bool = True):\n",
        "        # tokens: [B, T]\n",
        "        B, T = tokens.shape\n",
        "        x = self.tok_emb(tokens) + self.pos_emb[:T, :][None, ...]\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, deterministic=deterministic)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)  # [B, T, V]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device Mesh and Sharding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "\n",
        "def make_mesh():\n",
        "    devices = np.array(jax.devices())\n",
        "    if devices.size < 2:\n",
        "        raise RuntimeError(\"Need at least 2 devices for data parallelism.\")\n",
        "    return Mesh(devices, (\"data\",))\n",
        "\n",
        "def replicate_tree_to_mesh(tree, mesh):\n",
        "    return jax.tree_util.tree_map(\n",
        "        lambda x: jax.device_put(x, NamedSharding(mesh, P())),  # P() => replicate across 'data'\n",
        "        tree\n",
        "    )\n",
        "\n",
        "def shard_with_fsdp(tree, mesh):\n",
        "    def shard_array(x):\n",
        "        shape = x.shape\n",
        "        if len(shape) == 0:\n",
        "            return jax.device_put(x, NamedSharding(mesh, P()))\n",
        "        biggest_axis = int(np.argmax(shape))\n",
        "        spec = [None] * len(shape)\n",
        "        spec[biggest_axis] = \"data\"\n",
        "        return jax.device_put(x, NamedSharding(mesh, P(*spec)))\n",
        "\n",
        "    return jax.tree_util.tree_map(shard_array, tree)\n",
        "\n",
        "def shard_batch_to_mesh(batch_np, mesh, seq_len):\n",
        "    arr = jnp.asarray(batch_np, dtype=jnp.int32)\n",
        "    return jax.device_put(arr, NamedSharding(mesh, P(\"data\", None)))  # [B, T] -> shard B across 'data'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "\n",
        "def lm_loss_from_logits(logits, tokens):\n",
        "    # next-token prediction\n",
        "    targets = tokens[:, 1:]                   # [B, T-1]\n",
        "    logit_slice = logits[:, :-1, :]           # [B, T-1, V]\n",
        "    V = logit_slice.shape[-1]\n",
        "    onehot = jax.nn.one_hot(targets, V, dtype=jnp.float32)\n",
        "    logp = jax.nn.log_softmax(logit_slice, axis=-1)\n",
        "    # mean over batch and time\n",
        "    return -(onehot * logp).sum(-1).mean()\n",
        "\n",
        "def create_train_state(rng, cfg: TrainCfg):\n",
        "    model = TinyTransformerLM(\n",
        "        vocab_size=cfg.vocab_size, d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
        "        n_layers=cfg.n_layers, d_ff=cfg.d_ff, max_seq=cfg.seq_len\n",
        "    )\n",
        "    # Mock input for shape inference\n",
        "    dummy = jnp.zeros((8, cfg.seq_len), dtype=jnp.int32)\n",
        "    params = model.init(rng, dummy, deterministic=True)[\"params\"]\n",
        "    tx = optax.adamw(cfg.lr)\n",
        "    return TrainState(step=0, apply_fn=model.apply, params=params, tx=tx, opt_state=tx.init(params)), model\n",
        "\n",
        "\n",
        "def build_step_fn(cfg: TrainCfg, model: TinyTransformerLM):\n",
        "    def step(state: TrainState, tokens_sharded: jnp.ndarray):\n",
        "        tokens_local = lax.with_sharding_constraint(tokens_sharded, P(\"data\", None))\n",
        "\n",
        "        def loss_fn(params):\n",
        "            logits = state.apply_fn({\"params\": params}, tokens_local, deterministic=True)\n",
        "            loss = lm_loss_from_logits(logits, tokens_local)\n",
        "            return loss, logits\n",
        "\n",
        "        (loss, _), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "\n",
        "        new_state = state.apply_gradients(grads=grads)\n",
        "        return new_state, loss\n",
        "\n",
        "    step_jit = jax.jit(step, donate_argnums=(0,))\n",
        "    return step_jit\n",
        "\n",
        "def run(cfg: TrainCfg):\n",
        "    rng = random.PRNGKey(cfg.seed)\n",
        "    data = make_toy_dataset(cfg, rng_seed=1234)\n",
        "    mesh = make_mesh()\n",
        "\n",
        "    with jax.set_mesh(mesh):\n",
        "        state, model = create_train_state(rng, cfg)    \n",
        "\n",
        "    with jax.set_mesh(mesh):\n",
        "        # For FSDP, it's as simple as annotating params and opt_state\n",
        "        # state = state.replace(\n",
        "        #     params=shard_with_fsdp(state.params, mesh),\n",
        "        #     opt_state=shard_with_fsdp(state.opt_state, mesh),\n",
        "        # )\n",
        "        state = state.replace(\n",
        "            params=replicate_tree_to_mesh(state.params, mesh),\n",
        "            opt_state=replicate_tree_to_mesh(state.opt_state, mesh),\n",
        "        )\n",
        "        step_jit = build_step_fn(cfg, model)\n",
        "\n",
        "        # Training\n",
        "        it = batch_iter(data, cfg.batch_size)\n",
        "        for s in range(cfg.steps):\n",
        "            batch = next(it)\n",
        "            tokens_sharded = shard_batch_to_mesh(batch, mesh, cfg.seq_len)\n",
        "\n",
        "            state, loss = step_jit(state, tokens_sharded)\n",
        "            if s % 10 == 0 or s == cfg.steps - 1:\n",
        "                print(f\"[jax-dp] step {s:04d}  loss {float(loss):.4f}\")\n",
        "\n",
        "    params_host = jax.tree_util.tree_map(lambda x: np.array(x.addressable_data(0)), state.params)\n",
        "    return params_host\n",
        "\n",
        "\n",
        "def train(steps: int = 50, batch_size: int = 32, seq_len: int = 64, vocab_size: int = 512, seed: int = 42):\n",
        "    cfg = TrainCfg(\n",
        "        vocab_size=vocab_size,\n",
        "        seq_len=seq_len,\n",
        "        batch_size=batch_size,\n",
        "        steps=steps,\n",
        "        seed=seed,\n",
        "    )\n",
        "    run(cfg)\n",
        "\n",
        "train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jax-dp] step 0000  loss 6.7112\n",
            "[jax-dp] step 0010  loss 6.3130\n",
            "[jax-dp] step 0020  loss 6.2615\n",
            "[jax-dp] step 0030  loss 6.2571\n",
            "[jax-dp] step 0040  loss 6.2508\n",
            "[jax-dp] step 0049  loss 6.2567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}