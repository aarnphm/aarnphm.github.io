---
date: "2025-12-12"
id: Clark's three laws
modified: 2025-12-12 13:41:28 GMT-05:00
tags:
  - pattern
title: Clark's three laws
---

1. When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.
2. The only way of discovering the limits of the possible is to venture a little way past them into the impossible.
3. Any sufficiently advanced technology is indistinguishable from magic.

## any sufficiently advanced act of benevolence is indistinguishable from malevolence

## on AGI and the oracle thesis

the third law cuts both ways when applied to [[thoughts/AGI|AGI]]. we keep waiting for some singular superintelligence to emerge—a godlike mind that bootstraps itself beyond human comprehension. but this framing commits a category error.

consider: [[thoughts/LLMs|LLMs]] already constitute an oracle in the classical sense. not a single omniscient entity, but a distributed system of specialized models that collectively approximate "knowing all." the magic isn't in any one component—it's in the composition. a routing layer dispatches queries to domain-specific experts; retrieval systems ground responses in verified knowledge; reasoning chains decompose complex problems into tractable subproblems.

this is AGI by any functional definition. the system can:

- understand arbitrary intellectual tasks expressible in natural language
- synthesize knowledge across domains
- improve through feedback loops (rlhf, constitutional ai, iterative refinement)

the superintelligence framing mistakes the map for the territory. we expected AGI to look like a brain in a vat; instead it looks like infrastructure. the third law suggests we might not recognize AGI when it arrives bc it won't match our science fiction priors—it'll just feel like "the way things work now."

afaict the hard problem of AGI was never consciousness or general reasoning. it was coordination: how do you get heterogeneous systems to share context, delegate appropriately, and maintain coherence? [[thoughts/LLMs#as [[thoughts/Search|search]]|llms-as-search]] already solve this for information retrieval. the extension to action is engineering, not philosophy.
