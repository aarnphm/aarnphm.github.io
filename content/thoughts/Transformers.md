---
id: Transformers
tags:
  - ml
date: "2024-02-07"
title: Transformers
---

See also: [[thoughts/LLMs|LLMs]]

ELI5: Mom often creates a food list consists of $n$ of items to buy. Your job is to guess what the last item on this list would be.

Most implementations are [[thoughts/Autoregressive models|autoregressive]]. Most major SOTA are decoder-only, as encoder-decoder models has lack behind due to their expensive encoding phase.

See this amazing [visualisation from Brendan Bycroft](https://bbycroft.net/llm)

Currently, there is a rise for [[thoughts/state-space models|state-space models]] which shows promise in information-dense [[thoughts/data|data]]

## inference.

### [[thoughts/Embedding|embedding]]

### next-token prediction.

